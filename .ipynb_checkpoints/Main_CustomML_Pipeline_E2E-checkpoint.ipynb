{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4501cf18-2908-4527-87e6-e7e48e0446b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "USER_FLAG = \"--user\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1f245e3-29d1-4311-8663-05389e6e8ffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting google-cloud-aiplatform==1.7.0\n",
      "  Downloading google_cloud_aiplatform-1.7.0-py2.py3-none-any.whl (1.6 MB)\n",
      "     |████████████████████████████████| 1.6 MB 5.3 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: google-api-core[grpc]<3.0.0dev,>=1.26.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform==1.7.0) (2.3.2)\n",
      "Requirement already satisfied: google-cloud-storage<2.0.0dev,>=1.32.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform==1.7.0) (1.43.0)\n",
      "Requirement already satisfied: google-cloud-bigquery<3.0.0dev,>=1.15.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform==1.7.0) (2.31.0)\n",
      "Requirement already satisfied: proto-plus>=1.10.1 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform==1.7.0) (1.19.8)\n",
      "Requirement already satisfied: packaging>=14.3 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform==1.7.0) (21.3)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform==1.7.0) (59.6.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.52.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform==1.7.0) (1.53.0)\n",
      "Requirement already satisfied: google-auth<3.0dev,>=1.25.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform==1.7.0) (2.3.3)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform==1.7.0) (2.26.0)\n",
      "Requirement already satisfied: protobuf>=3.12.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform==1.7.0) (3.19.1)\n",
      "Requirement already satisfied: grpcio-status<2.0dev,>=1.33.2 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform==1.7.0) (1.43.0)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform==1.7.0) (1.43.0)\n",
      "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.4.1 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform==1.7.0) (2.2.1)\n",
      "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform==1.7.0) (2.1.0)\n",
      "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.2 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform==1.7.0) (2.8.2)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage<2.0.0dev,>=1.32.0->google-cloud-aiplatform==1.7.0) (1.16.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=14.3->google-cloud-aiplatform==1.7.0) (3.0.6)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform==1.7.0) (4.8)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform==1.7.0) (0.2.7)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform==1.7.0) (4.2.4)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.7/site-packages (from google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform==1.7.0) (1.1.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform==1.7.0) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform==1.7.0) (2.0.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform==1.7.0) (3.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform==1.7.0) (2021.10.8)\n",
      "Requirement already satisfied: cffi>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from google-crc32c<2.0dev,>=1.0->google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform==1.7.0) (1.15.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=1.25.0->google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform==1.7.0) (0.4.8)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.7/site-packages (from cffi>=1.0.0->google-crc32c<2.0dev,>=1.0->google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform==1.7.0) (2.21)\n",
      "Installing collected packages: google-cloud-aiplatform\n",
      "\u001b[33m  WARNING: The script tb-gcp-uploader is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "Successfully installed google-cloud-aiplatform-1.7.0\n",
      "Collecting kfp==1.8.9\n",
      "  Downloading kfp-1.8.9.tar.gz (296 kB)\n",
      "     |████████████████████████████████| 296 kB 5.4 MB/s            \n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting google-cloud-pipeline-components==0.2.0\n",
      "  Downloading google_cloud_pipeline_components-0.2.0-py3-none-any.whl (135 kB)\n",
      "     |████████████████████████████████| 135 kB 59.9 MB/s            \n",
      "\u001b[?25hCollecting absl-py<=0.11,>=0.9\n",
      "  Downloading absl_py-0.11.0-py3-none-any.whl (127 kB)\n",
      "     |████████████████████████████████| 127 kB 65.8 MB/s            \n",
      "\u001b[?25hCollecting PyYAML<6,>=5.3\n",
      "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
      "     |████████████████████████████████| 636 kB 63.2 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: google-cloud-storage<2,>=1.20.0 in /opt/conda/lib/python3.7/site-packages (from kfp==1.8.9) (1.43.0)\n",
      "Collecting kubernetes<19,>=8.0.0\n",
      "  Downloading kubernetes-18.20.0-py2.py3-none-any.whl (1.6 MB)\n",
      "     |████████████████████████████████| 1.6 MB 62.5 MB/s            \n",
      "\u001b[?25hCollecting google-api-python-client<2,>=1.7.8\n",
      "  Downloading google_api_python_client-1.12.8-py2.py3-none-any.whl (61 kB)\n",
      "     |████████████████████████████████| 61 kB 45 kB/s              \n",
      "\u001b[?25hCollecting google-auth<2,>=1.6.1\n",
      "  Downloading google_auth-1.35.0-py2.py3-none-any.whl (152 kB)\n",
      "     |████████████████████████████████| 152 kB 65.1 MB/s            \n",
      "\u001b[?25hCollecting requests-toolbelt<1,>=0.8.0\n",
      "  Downloading requests_toolbelt-0.9.1-py2.py3-none-any.whl (54 kB)\n",
      "     |████████████████████████████████| 54 kB 4.4 MB/s             \n",
      "\u001b[?25hRequirement already satisfied: cloudpickle<3,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from kfp==1.8.9) (2.0.0)\n",
      "Collecting kfp-server-api<2.0.0,>=1.1.2\n",
      "  Downloading kfp-server-api-1.7.1.tar.gz (52 kB)\n",
      "     |████████████████████████████████| 52 kB 2.3 MB/s             \n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting jsonschema<4,>=3.0.1\n",
      "  Downloading jsonschema-3.2.0-py2.py3-none-any.whl (56 kB)\n",
      "     |████████████████████████████████| 56 kB 7.0 MB/s             \n",
      "\u001b[?25hCollecting tabulate<1,>=0.8.6\n",
      "  Downloading tabulate-0.8.9-py3-none-any.whl (25 kB)\n",
      "Requirement already satisfied: click<9,>=7.1.2 in /opt/conda/lib/python3.7/site-packages (from kfp==1.8.9) (8.0.3)\n",
      "Collecting Deprecated<2,>=1.2.7\n",
      "  Downloading Deprecated-1.2.13-py2.py3-none-any.whl (9.6 kB)\n",
      "Collecting strip-hints<1,>=0.1.8\n",
      "  Downloading strip-hints-0.1.10.tar.gz (29 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting docstring-parser<1,>=0.7.3\n",
      "  Downloading docstring_parser-0.13.tar.gz (23 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting kfp-pipeline-spec<0.2.0,>=0.1.13\n",
      "  Downloading kfp_pipeline_spec-0.1.13-py3-none-any.whl (18 kB)\n",
      "Collecting fire<1,>=0.3.1\n",
      "  Downloading fire-0.4.0.tar.gz (87 kB)\n",
      "     |████████████████████████████████| 87 kB 9.4 MB/s             \n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: protobuf<4,>=3.13.0 in /opt/conda/lib/python3.7/site-packages (from kfp==1.8.9) (3.19.1)\n",
      "Collecting uritemplate<4,>=3.0.1\n",
      "  Downloading uritemplate-3.0.1-py2.py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: pydantic<2,>=1.8.2 in /opt/conda/lib/python3.7/site-packages (from kfp==1.8.9) (1.8.2)\n",
      "Collecting typer<1.0,>=0.3.2\n",
      "  Downloading typer-0.4.0-py3-none-any.whl (27 kB)\n",
      "Collecting typing-extensions<4,>=3.7.4\n",
      "  Downloading typing_extensions-3.10.0.2-py3-none-any.whl (26 kB)\n",
      "Collecting google-api-core<2dev,>=1.26.0\n",
      "  Downloading google_api_core-1.31.5-py2.py3-none-any.whl (93 kB)\n",
      "     |████████████████████████████████| 93 kB 2.4 MB/s             \n",
      "\u001b[?25hCollecting google-cloud-notebooks>=0.4.0\n",
      "  Downloading google_cloud_notebooks-1.1.1-py2.py3-none-any.whl (198 kB)\n",
      "     |████████████████████████████████| 198 kB 92.2 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: google-cloud-aiplatform>=1.4.3 in /home/jupyter/.local/lib/python3.7/site-packages (from google-cloud-pipeline-components==0.2.0) (1.7.0)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from absl-py<=0.11,>=0.9->kfp==1.8.9) (1.16.0)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from click<9,>=7.1.2->kfp==1.8.9) (4.9.0)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /opt/conda/lib/python3.7/site-packages (from Deprecated<2,>=1.2.7->kfp==1.8.9) (1.13.3)\n",
      "Requirement already satisfied: termcolor in /opt/conda/lib/python3.7/site-packages (from fire<1,>=0.3.1->kfp==1.8.9) (1.1.0)\n",
      "Requirement already satisfied: pytz in /opt/conda/lib/python3.7/site-packages (from google-api-core<2dev,>=1.26.0->google-cloud-pipeline-components==0.2.0) (2021.3)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2dev,>=1.26.0->google-cloud-pipeline-components==0.2.0) (59.6.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2dev,>=1.26.0->google-cloud-pipeline-components==0.2.0) (1.53.0)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2dev,>=1.26.0->google-cloud-pipeline-components==0.2.0) (2.26.0)\n",
      "Requirement already satisfied: packaging>=14.3 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2dev,>=1.26.0->google-cloud-pipeline-components==0.2.0) (21.3)\n",
      "Requirement already satisfied: httplib2<1dev,>=0.15.0 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client<2,>=1.7.8->kfp==1.8.9) (0.20.2)\n",
      "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client<2,>=1.7.8->kfp==1.8.9) (0.1.0)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.1->kfp==1.8.9) (4.2.4)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.1->kfp==1.8.9) (4.8)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.1->kfp==1.8.9) (0.2.7)\n",
      "Requirement already satisfied: google-api-core[grpc]<3.0.0dev,>=1.26.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform>=1.4.3->google-cloud-pipeline-components==0.2.0) (2.3.2)\n",
      "Requirement already satisfied: proto-plus>=1.10.1 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform>=1.4.3->google-cloud-pipeline-components==0.2.0) (1.19.8)\n",
      "Requirement already satisfied: google-cloud-bigquery<3.0.0dev,>=1.15.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform>=1.4.3->google-cloud-pipeline-components==0.2.0) (2.31.0)\n",
      "Requirement already satisfied: google-cloud-core<3.0dev,>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage<2,>=1.20.0->kfp==1.8.9) (2.2.1)\n",
      "Requirement already satisfied: google-resumable-media<3.0dev,>=1.3.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage<2,>=1.20.0->kfp==1.8.9) (2.1.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema<4,>=3.0.1->kfp==1.8.9) (0.18.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema<4,>=3.0.1->kfp==1.8.9) (21.2.0)\n",
      "Requirement already satisfied: urllib3>=1.15 in /opt/conda/lib/python3.7/site-packages (from kfp-server-api<2.0.0,>=1.1.2->kfp==1.8.9) (1.26.7)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.7/site-packages (from kfp-server-api<2.0.0,>=1.1.2->kfp==1.8.9) (2021.10.8)\n",
      "Requirement already satisfied: python-dateutil in /opt/conda/lib/python3.7/site-packages (from kfp-server-api<2.0.0,>=1.1.2->kfp==1.8.9) (2.8.2)\n",
      "Requirement already satisfied: requests-oauthlib in /opt/conda/lib/python3.7/site-packages (from kubernetes<19,>=8.0.0->kfp==1.8.9) (1.3.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /opt/conda/lib/python3.7/site-packages (from kubernetes<19,>=8.0.0->kfp==1.8.9) (1.2.3)\n",
      "Requirement already satisfied: wheel in /opt/conda/lib/python3.7/site-packages (from strip-hints<1,>=0.1.8->kfp==1.8.9) (0.37.0)\n",
      "Collecting google-api-core[grpc]<3.0.0dev,>=1.26.0\n",
      "  Downloading google_api_core-2.3.0-py2.py3-none-any.whl (109 kB)\n",
      "     |████████████████████████████████| 109 kB 70.0 MB/s            \n",
      "\u001b[?25h  Downloading google_api_core-2.2.2-py2.py3-none-any.whl (95 kB)\n",
      "     |████████████████████████████████| 95 kB 7.5 MB/s             \n",
      "\u001b[?25h  Downloading google_api_core-2.2.1-py2.py3-none-any.whl (95 kB)\n",
      "     |████████████████████████████████| 95 kB 6.8 MB/s             \n",
      "\u001b[?25h  Downloading google_api_core-2.2.0-py2.py3-none-any.whl (95 kB)\n",
      "     |████████████████████████████████| 95 kB 7.6 MB/s             \n",
      "\u001b[?25h  Downloading google_api_core-2.1.1-py2.py3-none-any.whl (95 kB)\n",
      "     |████████████████████████████████| 95 kB 5.9 MB/s             \n",
      "\u001b[?25h  Downloading google_api_core-2.1.0-py2.py3-none-any.whl (94 kB)\n",
      "     |████████████████████████████████| 94 kB 5.3 MB/s             \n",
      "\u001b[?25h  Downloading google_api_core-2.0.1-py2.py3-none-any.whl (92 kB)\n",
      "     |████████████████████████████████| 92 kB 699 kB/s             \n",
      "\u001b[?25h  Downloading google_api_core-2.0.0-py2.py3-none-any.whl (92 kB)\n",
      "     |████████████████████████████████| 92 kB 620 kB/s             \n",
      "\u001b[?25hRequirement already satisfied: grpcio<2.0dev,>=1.29.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2dev,>=1.26.0->google-cloud-pipeline-components==0.2.0) (1.43.0)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.7/site-packages (from google-resumable-media<3.0dev,>=1.3.0->google-cloud-storage<2,>=1.20.0->kfp==1.8.9) (1.1.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /opt/conda/lib/python3.7/site-packages (from httplib2<1dev,>=0.15.0->google-api-python-client<2,>=1.7.8->kfp==1.8.9) (3.0.6)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.1->kfp==1.8.9) (0.4.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.26.0->google-cloud-pipeline-components==0.2.0) (2.0.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.26.0->google-cloud-pipeline-components==0.2.0) (3.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->click<9,>=7.1.2->kfp==1.8.9) (3.6.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib->kubernetes<19,>=8.0.0->kfp==1.8.9) (3.1.1)\n",
      "Requirement already satisfied: cffi>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from google-crc32c<2.0dev,>=1.0->google-resumable-media<3.0dev,>=1.3.0->google-cloud-storage<2,>=1.20.0->kfp==1.8.9) (1.15.0)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.7/site-packages (from cffi>=1.0.0->google-crc32c<2.0dev,>=1.0->google-resumable-media<3.0dev,>=1.3.0->google-cloud-storage<2,>=1.20.0->kfp==1.8.9) (2.21)\n",
      "Building wheels for collected packages: kfp, docstring-parser, fire, kfp-server-api, strip-hints\n",
      "  Building wheel for kfp (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for kfp: filename=kfp-1.8.9-py3-none-any.whl size=409653 sha256=dd3d0a82b83c894bf908c2d6ea8764be8d8489c51025a972a76be5b04af9a44b\n",
      "  Stored in directory: /home/jupyter/.cache/pip/wheels/0e/20/7e/c2c43249eb0538c5aa2542bcc9b02affb0211ed5617fbd4abc\n",
      "  Building wheel for docstring-parser (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for docstring-parser: filename=docstring_parser-0.13-py3-none-any.whl size=31866 sha256=9067fadfacc28bf66e84da4419a3e87836807933518b17d5f85fc9c5419c6ac7\n",
      "  Stored in directory: /home/jupyter/.cache/pip/wheels/bd/88/3c/d1aa049309f7945178cac9fbe6561a86424f432da57c18ca0f\n",
      "  Building wheel for fire (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for fire: filename=fire-0.4.0-py2.py3-none-any.whl size=115943 sha256=882afbb4dbfa58a3f9c656a03d4d3aa5180c598aa8943496a6e06013c4a9991b\n",
      "  Stored in directory: /home/jupyter/.cache/pip/wheels/8a/67/fb/2e8a12fa16661b9d5af1f654bd199366799740a85c64981226\n",
      "  Building wheel for kfp-server-api (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for kfp-server-api: filename=kfp_server_api-1.7.1-py3-none-any.whl size=92618 sha256=fe93fadec7495e7d79bd3f145fe189a8bd38dd56f4ecce887f5a954ae3c2b5bc\n",
      "  Stored in directory: /home/jupyter/.cache/pip/wheels/68/3f/d5/734c0278dd6c8969cef359edcf059505a61452c5eb0e2760e1\n",
      "  Building wheel for strip-hints (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for strip-hints: filename=strip_hints-0.1.10-py2.py3-none-any.whl size=22302 sha256=99aeeb231cc5228bc306d0477c528fd09e098cfe7e181b0458ac63594e69ea0f\n",
      "  Stored in directory: /home/jupyter/.cache/pip/wheels/5e/14/c3/6e44e9b2545f2d570b03f5b6d38c00b7534aa8abb376978363\n",
      "Successfully built kfp docstring-parser fire kfp-server-api strip-hints\n",
      "Installing collected packages: typing-extensions, google-auth, google-api-core, uritemplate, PyYAML, typer, tabulate, strip-hints, requests-toolbelt, kubernetes, kfp-server-api, kfp-pipeline-spec, jsonschema, google-api-python-client, fire, docstring-parser, Deprecated, absl-py, kfp, google-cloud-notebooks, google-cloud-pipeline-components\n",
      "\u001b[33m  WARNING: The script tabulate is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  WARNING: The script strip-hints is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  WARNING: The script jsonschema is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  WARNING: The scripts dsl-compile, dsl-compile-v2 and kfp are installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "explainable-ai-sdk 1.3.2 requires xai-image-widget, which is not installed.\n",
      "tfx-bsl 1.5.0 requires pyarrow<6,>=1, but you have pyarrow 6.0.1 which is incompatible.\n",
      "tensorflow-transform 1.5.0 requires pyarrow<6,>=1, but you have pyarrow 6.0.1 which is incompatible.\n",
      "tensorflow-io 0.21.0 requires tensorflow<2.7.0,>=2.6.0, but you have tensorflow 2.7.0 which is incompatible.\n",
      "tensorflow-io 0.21.0 requires tensorflow-io-gcs-filesystem==0.21.0, but you have tensorflow-io-gcs-filesystem 0.23.1 which is incompatible.\n",
      "pandas-profiling 3.0.0 requires tangled-up-in-unicode==0.1.0, but you have tangled-up-in-unicode 0.2.0 which is incompatible.\n",
      "cloud-tpu-client 0.10 requires google-api-python-client==1.8.0, but you have google-api-python-client 1.12.8 which is incompatible.\n",
      "apache-beam 2.34.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.4 which is incompatible.\n",
      "apache-beam 2.34.0 requires httplib2<0.20.0,>=0.8, but you have httplib2 0.20.2 which is incompatible.\n",
      "apache-beam 2.34.0 requires pyarrow<6.0.0,>=0.15.1, but you have pyarrow 6.0.1 which is incompatible.\u001b[0m\n",
      "Successfully installed Deprecated-1.2.13 PyYAML-5.4.1 absl-py-0.11.0 docstring-parser-0.13 fire-0.4.0 google-api-core-1.31.5 google-api-python-client-1.12.8 google-auth-1.35.0 google-cloud-notebooks-1.1.1 google-cloud-pipeline-components-0.2.0 jsonschema-3.2.0 kfp-1.8.9 kfp-pipeline-spec-0.1.13 kfp-server-api-1.7.1 kubernetes-18.20.0 requests-toolbelt-0.9.1 strip-hints-0.1.10 tabulate-0.8.9 typer-0.4.0 typing-extensions-3.10.0.2 uritemplate-3.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip3 install {USER_FLAG} google-cloud-aiplatform==1.7.0 --upgrade\n",
    "!pip3 install {USER_FLAG} kfp==1.8.9 google-cloud-pipeline-components==0.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9452729c-7737-452b-887e-e25bf9b57416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFP SDK version: 1.8.9\n",
      "google_cloud_pipeline_components version: 0.2.0\n"
     ]
    }
   ],
   "source": [
    "!python3 -c \"import kfp; print('KFP SDK version: {}'.format(kfp.__version__))\"\n",
    "!python3 -c \"import google_cloud_pipeline_components; print('google_cloud_pipeline_components version: {}'.format(google_cloud_pipeline_components.__version__))\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d503651b-5c0c-4b7f-ad84-4c1e77bc3604",
   "metadata": {},
   "source": [
    "### Step 1 -> Build the trainer package code to be used in custom training operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8960f66-1efc-4ae9-a687-4943703715b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing task.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile task.py\n",
    "\n",
    "# Owner - Hasan Rafiq\n",
    "# Load the TensorBoard notebook extension\n",
    "\n",
    "import hypertune\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import argparse\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "  \n",
    "# Determine CSV, label, and key columns\n",
    "#Columns in training sheet -> Can have extra columns too\n",
    "CSV_COLUMNS = ['fare', 'trip_start_month', 'trip_start_hour', 'trip_start_day',\n",
    "       'pickup_latitude', 'pickup_longitude', 'dropoff_latitude',\n",
    "       'dropoff_longitude']\n",
    "LABEL_COLUMN = 'fare'\n",
    "\n",
    "# Set default values for each CSV column( Including Y column )\n",
    "DEFAULTS = [[0.0], ['1'], ['1'],['1'],[0.0],[0.0],[0.0],[0.0]]\n",
    "\n",
    "bins_lat = [41.66367065, 41.85934972, 41.87740612, 41.87925508, 41.88099447,\n",
    "       41.88498719, 41.88530002, 41.89204214, 41.89207263, 41.89265811,\n",
    "       41.89830587, 41.89960211, 41.90026569, 41.90741282, 41.92187746,\n",
    "       41.92926299, 41.9442266 , 41.95402765, 41.97907082, 42.02122359]\n",
    "\n",
    "bins_lon = [-87.9136246 , -87.76550161, -87.68751552, -87.67161972,\n",
    "       -87.66341641, -87.65599818, -87.65253448, -87.64629348,\n",
    "       -87.642649  , -87.63784421, -87.63330804, -87.63274649,\n",
    "       -87.63186395, -87.62887416, -87.62621491, -87.62519214,\n",
    "       -87.62099291, -87.62076287, -87.61886836, -87.54093551]\n",
    "\n",
    "RAW_DATA_FEATURE_SPEC = dict([\n",
    "        ('fare', tf.io.VarLenFeature(tf.float32)),\n",
    "        ('trip_start_month', tf.io.VarLenFeature(tf.string)),\n",
    "        ('trip_start_hour', tf.io.VarLenFeature(tf.string)),\n",
    "        ('trip_start_day', tf.io.VarLenFeature(tf.string)),\n",
    "        ('pickup_latitude', tf.io.FixedLenFeature([], tf.float32)),\n",
    "        ('pickup_longitude', tf.io.FixedLenFeature([], tf.float32)),\n",
    "        ('dropoff_latitude', tf.io.FixedLenFeature([], tf.float32)),\n",
    "        ('dropoff_longitude', tf.io.FixedLenFeature([], tf.float32)),\n",
    "        ])\n",
    "    \n",
    "###############################\n",
    "##Feature engineering functions\n",
    "def feature_engg_features(features):\n",
    "  #Add new features( Non-TFT transformation ) -> Just for study purposes\n",
    "  features['distance'] = ((features['pickup_latitude'] - features['dropoff_latitude'])**2 +  (features['pickup_longitude'] - features['dropoff_longitude'])**2)**0.5\n",
    "\n",
    "  return(features)\n",
    "\n",
    "#To be called from TF\n",
    "def feature_engg(features, label):\n",
    "  #Add new features\n",
    "  features = feature_engg_features(features)\n",
    "\n",
    "  return(features, label)\n",
    "\n",
    "###############################\n",
    "###Data Input pipeline function\n",
    "\n",
    "def make_input_fn(filename, mode, vnum_epochs = None, batch_size = 512):\n",
    "    def _input_fn(v_test=False):     \n",
    "        # Create list of files that match pattern\n",
    "        file_list = tf.io.gfile.glob(filename)\n",
    "\n",
    "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "            num_epochs = vnum_epochs # indefinitely\n",
    "        else:\n",
    "            num_epochs = 1 # end-of-input after this        \n",
    "        \n",
    "        # Create dataset from file list\n",
    "        dataset = tf.compat.v1.data.experimental.make_csv_dataset(file_list,\n",
    "                                                   batch_size=batch_size,\n",
    "                                                   column_names=CSV_COLUMNS,\n",
    "                                                   column_defaults=DEFAULTS,\n",
    "                                                   label_name=LABEL_COLUMN,\n",
    "                                                   num_epochs = num_epochs,\n",
    "                                                   num_parallel_reads=30)\n",
    "        \n",
    "        dataset = dataset.prefetch(buffer_size = batch_size)\n",
    "\n",
    "        #Feature engineering\n",
    "        dataset = dataset.map(feature_engg)\n",
    "\n",
    "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "            num_epochs = vnum_epochs # indefinitely\n",
    "            dataset = dataset.shuffle(buffer_size = batch_size)\n",
    "        else:\n",
    "            num_epochs = 1 # end-of-input after this\n",
    "\n",
    "        dataset = dataset.repeat(num_epochs)       \n",
    "        \n",
    "        #Begins - Uncomment for testing only -----------------------------------------------------<\n",
    "        if v_test == True:\n",
    "          print(next(dataset.__iter__()))\n",
    "          \n",
    "        #End - Uncomment for testing only -----------------------------------------------------<\n",
    "        return dataset\n",
    "    return _input_fn\n",
    "\n",
    "# Define feature columns(Including feature engineered ones )\n",
    "# These are the features which come from the TF Data pipeline\n",
    "def create_feature_cols():\n",
    "    #Keras format features\n",
    "    # k_pickup_longitude_scaled = tf.keras.Input(name='pickup_longitude_scaled', shape=(1,), dtype=tf.float32, sparse=False) #-> Sparse because VarLenFeature\n",
    "    # k_pickup_latitude_scaled = tf.keras.Input(name='pickup_latitude_scaled', shape=(1,), dtype=tf.float32, sparse=False) #-> Sparse because VarLenFeature\n",
    "    k_month = tf.keras.Input(name='trip_start_month', shape=(1,), dtype=tf.string, sparse=False)\n",
    "    k_hour  = tf.keras.Input(name='trip_start_hour', shape=(1,), dtype=tf.string, sparse=False)\n",
    "    k_day  = tf.keras.Input(name='trip_start_day', shape=(1,), dtype=tf.string, sparse=False)\n",
    "    k_picklat  = tf.keras.Input(name='pickup_latitude', shape=(1,), dtype=tf.float32, sparse=False)\n",
    "    k_picklon  = tf.keras.Input(name='pickup_longitude', shape=(1,), dtype=tf.float32, sparse=False)\n",
    "    k_droplat  = tf.keras.Input(name='dropoff_latitude', shape=(1,), dtype=tf.float32, sparse=False)\n",
    "    k_droplon  = tf.keras.Input(name='dropoff_longitude', shape=(1,), dtype=tf.float32, sparse=False)\n",
    "    k_distance  = tf.keras.Input(name='distance', shape=(1,), dtype=tf.float32, sparse=False)\n",
    "    keras_dict_input = {'trip_start_month': k_month, 'trip_start_hour': k_hour, 'trip_start_day' : k_day,\n",
    "                        'pickup_latitude': k_picklat, 'pickup_longitude': k_picklon,\n",
    "                        'dropoff_latitude': k_droplat, 'dropoff_longitude': k_droplon, 'distance' : k_distance,\n",
    "                        # 'pickup_longitude_scaled': k_pickup_longitude_scaled,\n",
    "                        # 'pickup_latitude_scaled' : k_pickup_latitude_scaled\n",
    "                        }\n",
    "\n",
    "    return({'K' : keras_dict_input})\n",
    "\n",
    "def create_keras_model(params, feature_cols):\n",
    "    METRICS = [\n",
    "            keras.metrics.RootMeanSquaredError(name='rmse')\n",
    "    ]\n",
    "\n",
    "    #Input layers\n",
    "    input_feats = []\n",
    "    for inp in feature_cols['K'].keys():\n",
    "      input_feats.append(feature_cols['K'][inp])\n",
    "\n",
    "    ##Input processing\n",
    "    ##https://keras.io/examples/structured_data/structured_data_classification_from_scratch/\n",
    "    ##https://github.com/tensorflow/community/blob/master/rfcs/20191212-keras-categorical-inputs.md\n",
    "\n",
    "    ##Handle categorical attributes( One-hot encoding )\n",
    "    cat_day = tf.keras.layers.experimental.preprocessing.StringLookup(vocabulary = ['0','1','2','3','4','5','6','7'], mask_token=None, oov_token = '0')(feature_cols['K']['trip_start_day'])\n",
    "    cat_day = tf.keras.layers.experimental.preprocessing.CategoryEncoding(num_tokens=8)(cat_day)\n",
    "\n",
    "    cat_hour = tf.keras.layers.experimental.preprocessing.StringLookup(vocabulary=['1','2','3','4','5','6','7','8'\n",
    "                                                                                      '9','10','11','12','13','14','15','16',\n",
    "                                                                                      '17','18','19','20','21','22','23','0'\n",
    "                                                                                      ], mask_token=None)(feature_cols['K']['trip_start_hour'])\n",
    "    cat_hour = tf.keras.layers.experimental.preprocessing.CategoryEncoding(num_tokens=24)(cat_hour)\n",
    "\n",
    "    cat_month = tf.keras.layers.experimental.preprocessing.StringLookup(vocabulary=['1','2','3','4','5','6','7','8'\n",
    "                                                                                      '9','10','11','12'], mask_token=None)(feature_cols['K']['trip_start_month'])\n",
    "    cat_month = tf.keras.layers.experimental.preprocessing.CategoryEncoding(num_tokens=12)(cat_month)\n",
    "\n",
    "    # cat_company = tf.keras.layers.experimental.preprocessing.StringLookup(vocabulary=df['company'].unique(), mask_token=None)(feature_cols['K']['company'])\n",
    "    # cat_company = tf.keras.layers.experimental.preprocessing.CategoryEncoding(num_tokens=len(df['company'].unique()))(cat_company)\n",
    "\n",
    "    ##Binning\n",
    "    bins_pickup_lat = tf.keras.layers.experimental.preprocessing.Discretization(bins = bins_lat)(feature_cols['K']['pickup_latitude'])\n",
    "    cat_pickup_lat = tf.keras.layers.experimental.preprocessing.CategoryEncoding(len(bins_lat)+1)(bins_pickup_lat)\n",
    "\n",
    "    bins_pickup_lon = tf.keras.layers.experimental.preprocessing.Discretization(bins = bins_lon)(feature_cols['K']['pickup_longitude'])\n",
    "    cat_pickup_lon = tf.keras.layers.experimental.preprocessing.CategoryEncoding(len(bins_lon)+1)(bins_pickup_lon)\n",
    "\n",
    "    bins_drop_lat = tf.keras.layers.experimental.preprocessing.Discretization(bins = bins_lat)(feature_cols['K']['dropoff_latitude'])\n",
    "    cat_drop_lat = tf.keras.layers.experimental.preprocessing.CategoryEncoding(len(bins_lat)+1)(bins_drop_lat)\n",
    "\n",
    "    bins_drop_lon = tf.keras.layers.experimental.preprocessing.Discretization(bins = bins_lon)(feature_cols['K']['dropoff_longitude'])\n",
    "    cat_drop_lon = tf.keras.layers.experimental.preprocessing.CategoryEncoding(len(bins_lon)+1)(bins_drop_lon)\n",
    "\n",
    "    ##Categorical cross\n",
    "    cross_day_hour = tf.keras.layers.experimental.preprocessing.CategoryCrossing()([cat_day, cat_hour])\n",
    "    # hash_cross_day_hour = tf.keras.layers.experimental.preprocessing.HashedCrossing(num_bins=24 * 8, output_mode='one_hot')(cross_day_hour)\n",
    "\n",
    "#     cross_pick_lon_lat = tf.keras.layers.experimental.preprocessing.CategoryCrossing()([cat_pickup_lat, cat_pickup_lon])\n",
    "#     hash_cross_pick_lon_lat = tf.keras.layers.experimental.preprocessing.HashedCrossing(num_bins=(len(bins_lat) + 1) ** 2)(cross_pick_lon_lat)\n",
    "\n",
    "#     cross_drop_lon_lat = tf.keras.layers.experimental.preprocessing.CategoryCrossing()([cat_drop_lat, cat_drop_lon])\n",
    "#     hash_cross_drop_lon_lat = tf.keras.layers.experimental.preprocessing.HashedCrossing(num_bins=(len(bins_lat) + 1) ** 2)(cross_drop_lon_lat)\n",
    "\n",
    "    # Cross to embedding\n",
    "#     embed_cross_pick_lon_lat = tf.keras.layers.Embedding(((len(bins_lat) + 1) ** 2), 4)(hash_cross_pick_lon_lat)\n",
    "#     embed_cross_pick_lon_lat = tf.reduce_sum(embed_cross_pick_lon_lat, axis=-2)\n",
    "\n",
    "#     embed_cross_drop_lon_lat = tf.keras.layers.Embedding(((len(bins_lat) + 1) ** 2), 4)(hash_cross_drop_lon_lat)\n",
    "#     embed_cross_drop_lon_lat = tf.reduce_sum(embed_cross_drop_lon_lat, axis=-2)\n",
    "\n",
    "    # Also pass time attributes as Deep signal( Cast to integer )\n",
    "    int_trip_start_day = tf.strings.to_number(feature_cols['K']['trip_start_day'], tf.float32)\n",
    "    int_trip_start_hour = tf.strings.to_number(feature_cols['K']['trip_start_hour'], tf.float32)\n",
    "    int_trip_start_month = tf.strings.to_number(feature_cols['K']['trip_start_month'], tf.float32)\n",
    "\n",
    "    #Add feature engineered columns - LAMBDA layer\n",
    "\n",
    "    ###Create MODEL\n",
    "    ####Concatenate all features( Numerical input )\n",
    "    x_input_numeric = tf.keras.layers.concatenate([\n",
    "                    feature_cols['K']['pickup_latitude'], feature_cols['K']['pickup_longitude'],\n",
    "                    feature_cols['K']['dropoff_latitude'], feature_cols['K']['dropoff_longitude'],\n",
    "                    # feature_cols['K']['pickup_latitude_scaled'], feature_cols['K']['pickup_longitude_scaled'],\n",
    "                    feature_cols['K']['distance'], \n",
    "                    # embed_cross_pick_lon_lat, embed_cross_drop_lon_lat,\n",
    "                    int_trip_start_day, int_trip_start_hour, int_trip_start_month\n",
    "                    ])\n",
    "\n",
    "    #DEEP - This Dense layer connects to input layer - Numeric Data\n",
    "    # x_numeric = tf.keras.layers.Dense(32, activation='selu', kernel_initializer=\"lecun_normal\")(x_input_numeric)\n",
    "    x_numeric = tf.keras.layers.BatchNormalization()(x_input_numeric)\n",
    "\n",
    "    ####Concatenate all Categorical features( Categorical converted )\n",
    "    x_categ = tf.keras.layers.concatenate([\n",
    "                    cat_month, #cat_cross_day_hour, \n",
    "                    cat_pickup_lat, cat_pickup_lon,\n",
    "                    cat_drop_lat, cat_drop_lon\n",
    "                    ])\n",
    "    \n",
    "    #WIDE - This Dense layer connects to input layer - Categorical Data\n",
    "    # x_categ = tf.keras.layers.Dense(32, activation='selu', kernel_initializer=\"lecun_normal\")(x_input_categ)\n",
    "\n",
    "    ####Concatenate both Wide and Deep layers\n",
    "    x = tf.keras.layers.concatenate([x_categ, x_numeric])\n",
    "\n",
    "    for l_ in range(params['hidden_layers']):\n",
    "        x = tf.keras.layers.Dense(32, activation='selu', kernel_initializer=\"lecun_normal\",\n",
    "                                  activity_regularizer=tf.keras.regularizers.l2(0.00001))(x)\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "\n",
    "    #Final Layer\n",
    "    out = tf.keras.layers.Dense(1, activation='linear')(x)\n",
    "    model = tf.keras.Model(input_feats, out)\n",
    "\n",
    "    #Set optimizer\n",
    "    opt = tf.keras.optimizers.Adam(lr= params['lr'])\n",
    "\n",
    "    #Compile model\n",
    "    model.compile(loss='mean_squared_error',  optimizer=opt, metrics = METRICS)\n",
    "\n",
    "    #Print Summary\n",
    "    print(model.summary())\n",
    "    return model\n",
    "\n",
    "def keras_train_and_evaluate(model, train_dataset, validation_dataset, epochs=100):\n",
    "  #Add callbacks\n",
    "  reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
    "                                patience=5, min_lr=0.00001, verbose = 1)\n",
    "  \n",
    "  tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"logs/\")\n",
    "\n",
    "  #Train and Evaluate\n",
    "  out = model.fit(train_dataset, \n",
    "                  validation_data = validation_dataset,\n",
    "                  epochs=epochs,\n",
    "                  # validation_steps = 3,   ###Keep this none for running evaluation on full EVAL data every epoch\n",
    "                  steps_per_epoch = 100,   ###Has to be passed - Cant help it :) [ Number of batches per epoch ]\n",
    "                  callbacks=[reduce_lr, #modelsave_callback, \n",
    "                             tensorboard_callback, \n",
    "                             keras.callbacks.EarlyStopping(patience=20, restore_best_weights=True, verbose=True)]\n",
    "                  )\n",
    "  \n",
    "  return (min(out.history['val_rmse']))\n",
    "  \n",
    "def main(args):\n",
    "    @tf.function\n",
    "    def serving(dropoff_latitude, dropoff_longitude, pickup_latitude, pickup_longitude, trip_start_day, trip_start_hour, trip_start_month):\n",
    "        #Params coming in request\n",
    "        features = {\n",
    "            'dropoff_latitude': dropoff_latitude,\n",
    "            'dropoff_longitude': dropoff_longitude,\n",
    "            'pickup_latitude': pickup_latitude,\n",
    "            'pickup_longitude': pickup_longitude,\n",
    "            'trip_start_day': trip_start_day,\n",
    "            'trip_start_hour': trip_start_hour,\n",
    "            'trip_start_month': trip_start_month\n",
    "        }\n",
    "\n",
    "        #Add TFT transformations\n",
    "        raw_features = {}\n",
    "        for key, val in features.items():\n",
    "          if key not in RAW_DATA_FEATURE_SPEC:\n",
    "            continue\n",
    "          if isinstance(RAW_DATA_FEATURE_SPEC[key], tf.io.VarLenFeature):\n",
    "            raw_features[key] = tf.RaggedTensor.from_tensor(\n",
    "                tf.expand_dims(val, -1)).to_sparse()\n",
    "            continue\n",
    "          raw_features[key] = val\n",
    "        # tft_new_features = tft_layer(raw_features)\n",
    "\n",
    "        # pickup_longitude_scaled = tft_new_features['pickup_longitude_scaled'] \n",
    "        # pickup_latitude_scaled = tft_new_features['pickup_latitude_scaled']\n",
    "        distance = ((features['pickup_latitude'] - features['dropoff_latitude'])**2 +  (features['pickup_longitude'] - features['dropoff_longitude'])**2)**0.5 ##tft_new_features['distance']\n",
    "\n",
    "        ##Feature engineering( calculate distance )\n",
    "        # distance = tf.cast( tf.sqrt((tf.abs(dropoff_latitude - pickup_latitude))**2 + (tf.abs(dropoff_longitude - pickup_longitude))**2), tf.float32)\n",
    "\n",
    "        #Params in request + New Feature engineering params\n",
    "        payload = {\n",
    "            'dropoff_latitude': dropoff_latitude,\n",
    "            'dropoff_longitude': dropoff_longitude,\n",
    "            'pickup_latitude': pickup_latitude,\n",
    "            'pickup_longitude': pickup_longitude,\n",
    "            'trip_start_day': trip_start_day,\n",
    "            'trip_start_hour': trip_start_hour,\n",
    "            'trip_start_month': trip_start_month,\n",
    "            'distance': distance,\n",
    "            # 'pickup_longitude_scaled': pickup_longitude_scaled,\n",
    "            # 'pickup_latitude_scaled': pickup_latitude_scaled,\n",
    "        }\n",
    "\n",
    "        ## Predict\n",
    "        ##IF THERE IS AN ERROR IN NUMBER OF PARAMS PASSED HERE OR DATA TYPE THEN IT GIVES ERROR, \"COULDN'T COMPUTE OUTPUT TENSOR\"\n",
    "        predictions = m_(payload)\n",
    "        return predictions\n",
    "\n",
    "    #####MAIN STARTS\n",
    "    ##Device Strategy\n",
    "    device = \"cpu\"\n",
    "    if device == \"tpu\":\n",
    "      resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])\n",
    "      tf.config.experimental_connect_to_cluster(resolver)\n",
    "      # This is the TPU initialization code that has to be at the beginning.\n",
    "      tf.tpu.experimental.initialize_tpu_system(resolver)\n",
    "      strategy = tf.distribute.experimental.TPUStrategy(resolver)\n",
    "    else:\n",
    "      strategy = tf.distribute.MultiWorkerMirroredStrategy()\n",
    "    \n",
    "    #Create FC\n",
    "    create_feature_cols()\n",
    "\n",
    "    #Create model\n",
    "    params_default = {\n",
    "        'lr' : args.lr,\n",
    "        'beta_1' : 0.99,\n",
    "        'beta_2' : 0.999,\n",
    "        'epsilon' : 1e-08,\n",
    "        'decay' : 0.01,\n",
    "        'hidden_layers' : args.hidden_layers\n",
    "    }\n",
    "\n",
    "    #Create dataset input functions\n",
    "    train_dataset = make_input_fn(filename = args.train_file,\n",
    "                        mode = tf.estimator.ModeKeys.TRAIN,\n",
    "                        batch_size = 128)()\n",
    "\n",
    "    validation_dataset = make_input_fn(filename = args.eval_file,\n",
    "                        mode = tf.estimator.ModeKeys.EVAL,\n",
    "                        batch_size = 512)()\n",
    "\n",
    "    m_ = create_keras_model(params = params_default, feature_cols = create_feature_cols())\n",
    "    # tf.keras.utils.plot_model(m_, show_shapes=True, rankdir=\"LR\")\n",
    "\n",
    "    #Train Model\n",
    "    rmse = keras_train_and_evaluate(m_, train_dataset, validation_dataset, args.epochs)\n",
    "    print(\"Final Val RMSE: \", rmse)\n",
    "    \n",
    "    #Report metrics for HPT\n",
    "    hpt = hypertune.HyperTune()\n",
    "    hpt.report_hyperparameter_tuning_metric(\n",
    "      hyperparameter_metric_tag='eval_rmse',\n",
    "      metric_value=rmse,\n",
    "      global_step=args.epochs)\n",
    "    \n",
    "    #Save model\n",
    "    serving = serving.get_concrete_function(trip_start_day=tf.TensorSpec([None,], dtype= tf.string, name='trip_start_day'), \n",
    "                                            trip_start_hour=tf.TensorSpec([None,], dtype= tf.string, name='trip_start_hour'),\n",
    "                                            trip_start_month=tf.TensorSpec([None], dtype= tf.string, name='trip_start_month'), \n",
    "                                            dropoff_latitude=tf.TensorSpec([None,], dtype= tf.float32, name='dropoff_latitude'),\n",
    "                                            dropoff_longitude=tf.TensorSpec([None,], dtype= tf.float32, name='dropoff_longitude'), \n",
    "                                            pickup_latitude=tf.TensorSpec([None,], dtype= tf.float32, name='pickup_latitude'),\n",
    "                                            pickup_longitude=tf.TensorSpec([None,], dtype= tf.float32, name='pickup_longitude')\n",
    "                                            )\n",
    "\n",
    "    print(\"Saving model...\")\n",
    "    version = \"1\"  #{'serving_default': call_output}\n",
    "    tf.saved_model.save(\n",
    "        m_,\n",
    "        args.model_save_location + version,\n",
    "        signatures=serving\n",
    "    )\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    ##Parse Arguments\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "      '--train_file', required=True, type=str, help='Training file')\n",
    "    parser.add_argument(\n",
    "      '--eval_file', required=True, type=str, help='Eval file')\n",
    "    parser.add_argument(\n",
    "      '--model_save_location', required=True, type=str, help='Model save location')\n",
    "    parser.add_argument(\n",
    "      '--epochs', required=False, type=int, help='Epochs', default=100)\n",
    "    parser.add_argument(\n",
    "      '--lr', required=False, type=float, help='Learning Rate', default=0.001)\n",
    "    parser.add_argument(\n",
    "      '--hidden_layers', required=False, type=int, help='Hidden layers', default=1)\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    #Run Main Trainer\n",
    "    main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "012e3d4b-62fb-4a7f-8cc9-b86546dad172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing setup.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile setup.py\n",
    "\n",
    "from setuptools import find_packages\n",
    "from setuptools import setup\n",
    "\n",
    "REQUIRED_PACKAGES = ['absl-py','pandas',\n",
    "                     'google-cloud','google-cloud-storage','google-cloud-firestore','google-api-python-client', 'google-auth']\n",
    "\n",
    "setup(\n",
    "    name='trainer',\n",
    "    version='0.1',\n",
    "    install_requires=REQUIRED_PACKAGES,\n",
    "    packages=find_packages(),\n",
    "    include_package_data=True,\n",
    "    description='Hasan - Vertex AI Taxi Trainer Job'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef446d98-9cef-4610-aed1-58158a8a0227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing __init__.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile __init__.py\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7ac2864-e381-4782-bc2e-8abcde58ba70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running sdist\n",
      "running egg_info\n",
      "creating trainer.egg-info\n",
      "writing trainer.egg-info/PKG-INFO\n",
      "writing dependency_links to trainer.egg-info/dependency_links.txt\n",
      "writing requirements to trainer.egg-info/requires.txt\n",
      "writing top-level names to trainer.egg-info/top_level.txt\n",
      "writing manifest file 'trainer.egg-info/SOURCES.txt'\n",
      "reading manifest file 'trainer.egg-info/SOURCES.txt'\n",
      "writing manifest file 'trainer.egg-info/SOURCES.txt'\n",
      "running check\n",
      "warning: check: missing required meta-data: url\n",
      "\n",
      "warning: check: missing meta-data: either (author and author_email) or (maintainer and maintainer_email) must be supplied\n",
      "\n",
      "creating trainer-0.1\n",
      "creating trainer-0.1/components\n",
      "creating trainer-0.1/pipelines\n",
      "creating trainer-0.1/trainer\n",
      "creating trainer-0.1/trainer.egg-info\n",
      "copying files to trainer-0.1...\n",
      "copying README.md -> trainer-0.1\n",
      "copying setup.py -> trainer-0.1\n",
      "copying components/__init__.py -> trainer-0.1/components\n",
      "copying components/classification_eval_model.py -> trainer-0.1/components\n",
      "copying pipelines/__init__.py -> trainer-0.1/pipelines\n",
      "copying pipelines/train_pipeline.py -> trainer-0.1/pipelines\n",
      "copying trainer/__init__.py -> trainer-0.1/trainer\n",
      "copying trainer/task.py -> trainer-0.1/trainer\n",
      "copying trainer.egg-info/PKG-INFO -> trainer-0.1/trainer.egg-info\n",
      "copying trainer.egg-info/SOURCES.txt -> trainer-0.1/trainer.egg-info\n",
      "copying trainer.egg-info/dependency_links.txt -> trainer-0.1/trainer.egg-info\n",
      "copying trainer.egg-info/requires.txt -> trainer-0.1/trainer.egg-info\n",
      "copying trainer.egg-info/top_level.txt -> trainer-0.1/trainer.egg-info\n",
      "Writing trainer-0.1/setup.cfg\n",
      "creating dist\n",
      "Creating tar archive\n",
      "removing 'trainer-0.1' (and everything under it)\n",
      "Copying file://dist/trainer-0.1.tar.gz [Content-Type=application/x-tar]...\n",
      "/ [1 files][  9.2 KiB/  9.2 KiB]                                                \n",
      "Operation completed over 1 objects/9.2 KiB.                                      \n"
     ]
    }
   ],
   "source": [
    " # Create ML train package\n",
    "!rm -rf dist*\n",
    "!rm -rf trainer*\n",
    "!rm -rf trainer/\n",
    "!rm -rf dist/\n",
    "\n",
    "!mkdir trainer/\n",
    "!cp task.py trainer/\n",
    "!cp __init__.py trainer/\n",
    "!python setup.py sdist\n",
    "\n",
    "# Copy trainer.gz to GCS training path\n",
    "!gsutil cp dist/trainer-0.1.tar.gz gs://gcs-hasanrafiq-test-331814/ml_data/taxi_dataset/ml_scripts/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7de4c4-0b25-49c8-946c-30ec28faba4c",
   "metadata": {},
   "source": [
    "### Step 2 -> Now build pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17ab2d0c-c027-43c6-9ef6-15cc73242b45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting pipelines/train_pipeline.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile pipelines/train_pipeline.py\n",
    "import os\n",
    "\n",
    "import kfp\n",
    "import time\n",
    "from kfp.v2 import compiler, dsl\n",
    "from kfp.v2.dsl import component, pipeline, Artifact, ClassificationMetrics, Input, Output, Model, Metrics\n",
    "\n",
    "from google.cloud import aiplatform\n",
    "from google_cloud_pipeline_components import aiplatform as gcc_aip\n",
    "from typing import NamedTuple\n",
    "\n",
    "#Main pipeline class\n",
    "class pipeline_controller():\n",
    "    def __init__(self, template_path, display_name, pipeline_root, project_id, region):\n",
    "        self.template_path = template_path\n",
    "        self.display_name = display_name\n",
    "        self.pipeline_root = pipeline_root\n",
    "        self.project_id = project_id\n",
    "        self.region = region\n",
    "    \n",
    "    def _build_compile_pipeline(self):\n",
    "        \"\"\"Method to build and compile pipeline\"\"\"\n",
    "        self.pipeline = self._get_pipeline()\n",
    "        compiler.Compiler().compile(\n",
    "            pipeline_func=self.pipeline, package_path=self.template_path\n",
    "        )\n",
    "        \n",
    "        ##Write JSON file to GCS\n",
    "        \n",
    "    def _submit_job(self):\n",
    "        \"\"\"Method to Submit ML Pipeline job\"\"\"\n",
    "        #Next, define the job:\n",
    "        ml_pipeline_job = aiplatform.PipelineJob(\n",
    "            display_name=self.display_name,\n",
    "            template_path=self.template_path,\n",
    "            pipeline_root=self.pipeline_root,\n",
    "            parameter_values={\"project\": self.project_id, \"display_name\": self.display_name},\n",
    "            enable_caching=True\n",
    "        )\n",
    "\n",
    "        #And finally, run the job:\n",
    "        ml_pipeline_job.submit()\n",
    "    \n",
    "    def _get_pipeline(self):\n",
    "        \"\"\"Main method to Create pipeline\"\"\"\n",
    "        @pipeline(name=self.display_name,\n",
    "                          pipeline_root=self.pipeline_root)\n",
    "        def pipeline_fn(\n",
    "            display_name: str = self.display_name,\n",
    "            project: str = self.project_id,\n",
    "            gcp_region: str = self.region,\n",
    "            api_endpoint: str = \"us-central1-aiplatform.googleapis.com\",\n",
    "            thresholds_dict_str: str = '{\"auRoc\": 0.95}',\n",
    "        ):\n",
    "            \n",
    "            #Load all reusable custom components\n",
    "            #Option 1 -> Python function component wrapped as reusable function\n",
    "            # eval_op = kfp.components.load_component('component_specs/classification_eval_model.yaml')\n",
    "            \n",
    "            #Option 2 -> Python component wrapped as reusable package( preferred )\n",
    "            # eval_op = kfp.components.load_component('component_specs/classification_eval_model_v2.yaml')\n",
    "\n",
    "            #STEP: For non Auto-ML call\n",
    "            training_args = ['--train_file', 'gs://gcs-hasanrafiq-test-331814/ml_data/taxi_dataset/train.csv',\n",
    "                             '--eval_file', 'gs://gcs-hasanrafiq-test-331814/ml_data/taxi_dataset/eval.csv',\n",
    "                             '--model_save_location', 'gs://gcs-hasanrafiq-test-331814/ml_data/taxi_dataset/model/',\n",
    "                             '--epochs', '10',\n",
    "                             '--hidden_layers','2'\n",
    "                            ]\n",
    "\n",
    "            training_op = gcc_aip.CustomPythonPackageTrainingJobRunOp(\n",
    "                            project=project,\n",
    "                            display_name=display_name,\n",
    "                            python_package_gcs_uri=\"gs://gcs-hasanrafiq-test-331814/ml_data/taxi_dataset/ml_scripts/trainer-0.1.tar.gz\",\n",
    "                            staging_bucket='gs://cloud-ai-platform-35f2698c-5046-4c70-857e-14cb44e3950a/ml_staging',\n",
    "                            # base_output_dir='gs://cloud-ai-platform-35f2698c-5046-4c70-857e-14cb44e3950a/ml_staging',\n",
    "                            python_module_name=\"trainer.task\",\n",
    "                            container_uri='us-docker.pkg.dev/vertex-ai/training/tf-cpu.2-7:latest',\n",
    "                            replica_count=1,\n",
    "                            location=gcp_region,\n",
    "                            machine_type=\"n1-standard-4\",\n",
    "                            args=training_args\n",
    "                          )\n",
    "\n",
    "#             model_eval_task = eval_op(\n",
    "#                 project,\n",
    "#                 gcp_region,\n",
    "#                 api_endpoint,\n",
    "#                 thresholds_dict_str,\n",
    "#                 training_op.outputs[\"model\"],\n",
    "#             )\n",
    "\n",
    "#             with dsl.Condition(\n",
    "#                 model_eval_task.outputs[\"dep_decision\"] == \"true\",\n",
    "#                 name=\"deploy_decision\",\n",
    "#             ):\n",
    "\n",
    "#                 endpoint_op = gcc_aip.EndpointCreateOp(\n",
    "#                     project=project,\n",
    "#                     location=gcp_region,\n",
    "#                     display_name=\"train-automl-beans\",\n",
    "#                 )\n",
    "\n",
    "#                 gcc_aip.ModelDeployOp(\n",
    "#                     model=training_op.outputs[\"model\"],\n",
    "#                     endpoint=endpoint_op.outputs[\"endpoint\"],\n",
    "#                     dedicated_resources_min_replica_count=1,\n",
    "#                     dedicated_resources_max_replica_count=1,\n",
    "#                     dedicated_resources_machine_type=\"n1-standard-4\",\n",
    "#                 )\n",
    "            \n",
    "        return pipeline_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c8e2ba-3917-4ea5-be1c-ea36b254dcfd",
   "metadata": {},
   "source": [
    "##Directly test running of pipeline from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14582539-343f-4be7-b4e1-82de3606b33a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:Creating PipelineJob\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob created. Resource name: projects/318948681665/locations/us-central1/pipelineJobs/customml-taxi-training-20211229174319\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:To use this PipelineJob in another session:\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:pipeline_job = aiplatform.PipelineJob.get('projects/318948681665/locations/us-central1/pipelineJobs/customml-taxi-training-20211229174319')\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/customml-taxi-training-20211229174319?project=318948681665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.7/site-packages/kfp/v2/compiler/compiler.py:1266: FutureWarning: APIs imported from the v1 namespace (e.g. kfp.dsl, kfp.components, etc) will not be supported by the v2 compiler since v2.0.0\n",
      "  category=FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "#Initialize pipeline object\n",
    "from pipelines.train_pipeline import pipeline_controller\n",
    "import time\n",
    "\n",
    "PROJECT_ID = \"hasanrafiq-test-331814\"\n",
    "REGION=\"us-central1\"\n",
    "\n",
    "BUCKET_NAME=f\"gs://gcs-{PROJECT_ID}\"\n",
    "BUCKET_NAME\n",
    "\n",
    "PIPELINE_ROOT = f\"{BUCKET_NAME}/pipeline_root/\"\n",
    "PIPELINE_ROOT\n",
    "\n",
    "DISPLAY_NAME = 'hasantest-vertex-customml-pipeline{}'.format(str(int(time.time())))\n",
    "DISPLAY_NAME\n",
    "\n",
    "pipe = pipeline_controller(template_path=\"tab_classif_pipeline.json\",\n",
    "                           display_name=\"customml-taxi-training\", \n",
    "                           pipeline_root=PIPELINE_ROOT,\n",
    "                           project_id=PROJECT_ID,\n",
    "                           region=REGION)\n",
    "\n",
    "#Build and Compile pipeline\n",
    "pipe._build_compile_pipeline()\n",
    "\n",
    "# #Submit Job\n",
    "pipe._submit_job()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "11c29409-f32d-4b46-8c11-4d57bd014917",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compiler.Compiler().compile(\n",
    "#     pipeline_func=pipeline, package_path=\"tab_classif_pipeline.json\"\n",
    "# )\n",
    "\n",
    "# #Next, define the job:\n",
    "# ml_pipeline_job = aiplatform.PipelineJob(\n",
    "#     display_name=\"automl-tab-beans-training\",\n",
    "#     template_path=\"tab_classif_pipeline.json\",\n",
    "#     pipeline_root=PIPELINE_ROOT,\n",
    "#     parameter_values={\"project\": PROJECT_ID, \"display_name\": DISPLAY_NAME},\n",
    "#     enable_caching=True\n",
    "# )\n",
    "\n",
    "# #And finally, run the job:\n",
    "# ml_pipeline_job.submit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646bef8b-6737-4c25-b6eb-7376a9d71915",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-7.m87",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-7:m87"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
