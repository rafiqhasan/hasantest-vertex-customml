{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4501cf18-2908-4527-87e6-e7e48e0446b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "USER_FLAG = \"--user\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1f245e3-29d1-4311-8663-05389e6e8ffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-cloud-aiplatform==1.7.0 in /home/jupyter/.local/lib/python3.7/site-packages (1.7.0)\n",
      "Requirement already satisfied: google-api-core[grpc]<3.0.0dev,>=1.26.0 in /home/jupyter/.local/lib/python3.7/site-packages (from google-cloud-aiplatform==1.7.0) (1.31.5)\n",
      "Requirement already satisfied: packaging>=14.3 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform==1.7.0) (21.3)\n",
      "Requirement already satisfied: google-cloud-bigquery<3.0.0dev,>=1.15.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform==1.7.0) (2.31.0)\n",
      "Requirement already satisfied: proto-plus>=1.10.1 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform==1.7.0) (1.19.8)\n",
      "Requirement already satisfied: google-cloud-storage<2.0.0dev,>=1.32.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform==1.7.0) (1.43.0)\n",
      "Requirement already satisfied: six>=1.13.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform==1.7.0) (1.16.0)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform==1.7.0) (2.26.0)\n",
      "Requirement already satisfied: protobuf>=3.12.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform==1.7.0) (3.19.1)\n",
      "Requirement already satisfied: pytz in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform==1.7.0) (2021.3)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform==1.7.0) (59.6.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform==1.7.0) (1.53.0)\n",
      "Requirement already satisfied: google-auth<2.0dev,>=1.25.0 in /home/jupyter/.local/lib/python3.7/site-packages (from google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform==1.7.0) (1.35.0)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.29.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform==1.7.0) (1.43.0)\n",
      "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.4.1 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform==1.7.0) (2.2.1)\n",
      "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform==1.7.0) (2.1.0)\n",
      "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.2 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform==1.7.0) (2.8.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=14.3->google-cloud-aiplatform==1.7.0) (3.0.6)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<2.0dev,>=1.25.0->google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform==1.7.0) (4.2.4)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<2.0dev,>=1.25.0->google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform==1.7.0) (4.8)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<2.0dev,>=1.25.0->google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform==1.7.0) (0.2.7)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.7/site-packages (from google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform==1.7.0) (1.1.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform==1.7.0) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform==1.7.0) (2.0.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform==1.7.0) (3.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform==1.7.0) (1.26.7)\n",
      "Requirement already satisfied: cffi>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from google-crc32c<2.0dev,>=1.0->google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform==1.7.0) (1.15.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2.0dev,>=1.25.0->google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform==1.7.0) (0.4.8)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.7/site-packages (from cffi>=1.0.0->google-crc32c<2.0dev,>=1.0->google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform==1.7.0) (2.21)\n",
      "Requirement already satisfied: kfp==1.8.9 in /home/jupyter/.local/lib/python3.7/site-packages (1.8.9)\n",
      "Requirement already satisfied: google-cloud-pipeline-components==0.2.0 in /home/jupyter/.local/lib/python3.7/site-packages (0.2.0)\n",
      "Requirement already satisfied: google-cloud-storage<2,>=1.20.0 in /opt/conda/lib/python3.7/site-packages (from kfp==1.8.9) (1.43.0)\n",
      "Requirement already satisfied: pydantic<2,>=1.8.2 in /opt/conda/lib/python3.7/site-packages (from kfp==1.8.9) (1.8.2)\n",
      "Requirement already satisfied: kubernetes<19,>=8.0.0 in /home/jupyter/.local/lib/python3.7/site-packages (from kfp==1.8.9) (18.20.0)\n",
      "Requirement already satisfied: docstring-parser<1,>=0.7.3 in /home/jupyter/.local/lib/python3.7/site-packages (from kfp==1.8.9) (0.13)\n",
      "Requirement already satisfied: click<9,>=7.1.2 in /opt/conda/lib/python3.7/site-packages (from kfp==1.8.9) (8.0.3)\n",
      "Requirement already satisfied: typer<1.0,>=0.3.2 in /home/jupyter/.local/lib/python3.7/site-packages (from kfp==1.8.9) (0.4.0)\n",
      "Requirement already satisfied: protobuf<4,>=3.13.0 in /opt/conda/lib/python3.7/site-packages (from kfp==1.8.9) (3.19.1)\n",
      "Requirement already satisfied: absl-py<=0.11,>=0.9 in /home/jupyter/.local/lib/python3.7/site-packages (from kfp==1.8.9) (0.11.0)\n",
      "Requirement already satisfied: cloudpickle<3,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from kfp==1.8.9) (2.0.0)\n",
      "Requirement already satisfied: typing-extensions<4,>=3.7.4 in /home/jupyter/.local/lib/python3.7/site-packages (from kfp==1.8.9) (3.10.0.2)\n",
      "Requirement already satisfied: google-api-python-client<2,>=1.7.8 in /home/jupyter/.local/lib/python3.7/site-packages (from kfp==1.8.9) (1.12.8)\n",
      "Requirement already satisfied: fire<1,>=0.3.1 in /home/jupyter/.local/lib/python3.7/site-packages (from kfp==1.8.9) (0.4.0)\n",
      "Requirement already satisfied: strip-hints<1,>=0.1.8 in /home/jupyter/.local/lib/python3.7/site-packages (from kfp==1.8.9) (0.1.10)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.1 in /home/jupyter/.local/lib/python3.7/site-packages (from kfp==1.8.9) (1.35.0)\n",
      "Requirement already satisfied: requests-toolbelt<1,>=0.8.0 in /home/jupyter/.local/lib/python3.7/site-packages (from kfp==1.8.9) (0.9.1)\n",
      "Requirement already satisfied: uritemplate<4,>=3.0.1 in /home/jupyter/.local/lib/python3.7/site-packages (from kfp==1.8.9) (3.0.1)\n",
      "Requirement already satisfied: tabulate<1,>=0.8.6 in /home/jupyter/.local/lib/python3.7/site-packages (from kfp==1.8.9) (0.8.9)\n",
      "Requirement already satisfied: PyYAML<6,>=5.3 in /home/jupyter/.local/lib/python3.7/site-packages (from kfp==1.8.9) (5.4.1)\n",
      "Requirement already satisfied: Deprecated<2,>=1.2.7 in /home/jupyter/.local/lib/python3.7/site-packages (from kfp==1.8.9) (1.2.13)\n",
      "Requirement already satisfied: kfp-server-api<2.0.0,>=1.1.2 in /home/jupyter/.local/lib/python3.7/site-packages (from kfp==1.8.9) (1.7.1)\n",
      "Requirement already satisfied: kfp-pipeline-spec<0.2.0,>=0.1.13 in /home/jupyter/.local/lib/python3.7/site-packages (from kfp==1.8.9) (0.1.13)\n",
      "Requirement already satisfied: jsonschema<4,>=3.0.1 in /home/jupyter/.local/lib/python3.7/site-packages (from kfp==1.8.9) (3.2.0)\n",
      "Requirement already satisfied: google-cloud-notebooks>=0.4.0 in /home/jupyter/.local/lib/python3.7/site-packages (from google-cloud-pipeline-components==0.2.0) (1.1.1)\n",
      "Requirement already satisfied: google-cloud-aiplatform>=1.4.3 in /home/jupyter/.local/lib/python3.7/site-packages (from google-cloud-pipeline-components==0.2.0) (1.7.0)\n",
      "Requirement already satisfied: google-api-core<2dev,>=1.26.0 in /home/jupyter/.local/lib/python3.7/site-packages (from google-cloud-pipeline-components==0.2.0) (1.31.5)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from absl-py<=0.11,>=0.9->kfp==1.8.9) (1.16.0)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from click<9,>=7.1.2->kfp==1.8.9) (4.9.0)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /opt/conda/lib/python3.7/site-packages (from Deprecated<2,>=1.2.7->kfp==1.8.9) (1.13.3)\n",
      "Requirement already satisfied: termcolor in /opt/conda/lib/python3.7/site-packages (from fire<1,>=0.3.1->kfp==1.8.9) (1.1.0)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2dev,>=1.26.0->google-cloud-pipeline-components==0.2.0) (2.26.0)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2dev,>=1.26.0->google-cloud-pipeline-components==0.2.0) (59.6.0)\n",
      "Requirement already satisfied: packaging>=14.3 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2dev,>=1.26.0->google-cloud-pipeline-components==0.2.0) (21.3)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2dev,>=1.26.0->google-cloud-pipeline-components==0.2.0) (1.53.0)\n",
      "Requirement already satisfied: pytz in /opt/conda/lib/python3.7/site-packages (from google-api-core<2dev,>=1.26.0->google-cloud-pipeline-components==0.2.0) (2021.3)\n",
      "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client<2,>=1.7.8->kfp==1.8.9) (0.1.0)\n",
      "Requirement already satisfied: httplib2<1dev,>=0.15.0 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client<2,>=1.7.8->kfp==1.8.9) (0.20.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.1->kfp==1.8.9) (0.2.7)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.1->kfp==1.8.9) (4.2.4)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.1->kfp==1.8.9) (4.8)\n",
      "Requirement already satisfied: proto-plus>=1.10.1 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform>=1.4.3->google-cloud-pipeline-components==0.2.0) (1.19.8)\n",
      "Requirement already satisfied: google-cloud-bigquery<3.0.0dev,>=1.15.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform>=1.4.3->google-cloud-pipeline-components==0.2.0) (2.31.0)\n",
      "Requirement already satisfied: google-resumable-media<3.0dev,>=1.3.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage<2,>=1.20.0->kfp==1.8.9) (2.1.0)\n",
      "Requirement already satisfied: google-cloud-core<3.0dev,>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage<2,>=1.20.0->kfp==1.8.9) (2.2.1)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema<4,>=3.0.1->kfp==1.8.9) (0.18.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema<4,>=3.0.1->kfp==1.8.9) (21.2.0)\n",
      "Requirement already satisfied: urllib3>=1.15 in /opt/conda/lib/python3.7/site-packages (from kfp-server-api<2.0.0,>=1.1.2->kfp==1.8.9) (1.26.7)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.7/site-packages (from kfp-server-api<2.0.0,>=1.1.2->kfp==1.8.9) (2021.10.8)\n",
      "Requirement already satisfied: python-dateutil in /opt/conda/lib/python3.7/site-packages (from kfp-server-api<2.0.0,>=1.1.2->kfp==1.8.9) (2.8.2)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /opt/conda/lib/python3.7/site-packages (from kubernetes<19,>=8.0.0->kfp==1.8.9) (1.2.3)\n",
      "Requirement already satisfied: requests-oauthlib in /opt/conda/lib/python3.7/site-packages (from kubernetes<19,>=8.0.0->kfp==1.8.9) (1.3.0)\n",
      "Requirement already satisfied: wheel in /opt/conda/lib/python3.7/site-packages (from strip-hints<1,>=0.1.8->kfp==1.8.9) (0.37.0)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.29.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2dev,>=1.26.0->google-cloud-pipeline-components==0.2.0) (1.43.0)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.7/site-packages (from google-resumable-media<3.0dev,>=1.3.0->google-cloud-storage<2,>=1.20.0->kfp==1.8.9) (1.1.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /opt/conda/lib/python3.7/site-packages (from httplib2<1dev,>=0.15.0->google-api-python-client<2,>=1.7.8->kfp==1.8.9) (3.0.6)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.1->kfp==1.8.9) (0.4.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.26.0->google-cloud-pipeline-components==0.2.0) (2.0.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.26.0->google-cloud-pipeline-components==0.2.0) (3.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->click<9,>=7.1.2->kfp==1.8.9) (3.6.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib->kubernetes<19,>=8.0.0->kfp==1.8.9) (3.1.1)\n",
      "Requirement already satisfied: cffi>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from google-crc32c<2.0dev,>=1.0->google-resumable-media<3.0dev,>=1.3.0->google-cloud-storage<2,>=1.20.0->kfp==1.8.9) (1.15.0)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.7/site-packages (from cffi>=1.0.0->google-crc32c<2.0dev,>=1.0->google-resumable-media<3.0dev,>=1.3.0->google-cloud-storage<2,>=1.20.0->kfp==1.8.9) (2.21)\n",
      "Requirement already satisfied: cloudml-hypertune in /opt/conda/lib/python3.7/site-packages (0.1.0.dev6)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install google-cloud-aiplatform==1.7.0 --upgrade\n",
    "!pip3 install kfp==1.8.9 google-cloud-pipeline-components==0.2.0\n",
    "!pip3 install cloudml-hypertune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9452729c-7737-452b-887e-e25bf9b57416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFP SDK version: 1.8.9\n",
      "google_cloud_pipeline_components version: 0.2.0\n"
     ]
    }
   ],
   "source": [
    "!python3 -c \"import kfp; print('KFP SDK version: {}'.format(kfp.__version__))\"\n",
    "!python3 -c \"import google_cloud_pipeline_components; print('google_cloud_pipeline_components version: {}'.format(google_cloud_pipeline_components.__version__))\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d503651b-5c0c-4b7f-ad84-4c1e77bc3604",
   "metadata": {},
   "source": [
    "### Step 1 -> Build the trainer package code to be used in custom training operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8960f66-1efc-4ae9-a687-4943703715b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting task.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile task.py\n",
    "\n",
    "# Owner - Hasan Rafiq\n",
    "# Load the TensorBoard notebook extension\n",
    "\n",
    "import hypertune\n",
    "import pandas as pd\n",
    "import os\n",
    "from google.cloud import aiplatform\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import argparse\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "  \n",
    "# Determine CSV, label, and key columns\n",
    "#Columns in training sheet -> Can have extra columns too\n",
    "CSV_COLUMNS = ['fare', 'trip_start_month', 'trip_start_hour', 'trip_start_day',\n",
    "       'pickup_latitude', 'pickup_longitude', 'dropoff_latitude',\n",
    "       'dropoff_longitude']\n",
    "LABEL_COLUMN = 'fare'\n",
    "\n",
    "# Set default values for each CSV column( Including Y column )\n",
    "DEFAULTS = [[0.0], ['1'], ['1'],['1'],[0.0],[0.0],[0.0],[0.0]]\n",
    "\n",
    "bins_lat = [41.66367065, 41.85934972, 41.87740612, 41.87925508, 41.88099447,\n",
    "       41.88498719, 41.88530002, 41.89204214, 41.89207263, 41.89265811,\n",
    "       41.89830587, 41.89960211, 41.90026569, 41.90741282, 41.92187746,\n",
    "       41.92926299, 41.9442266 , 41.95402765, 41.97907082, 42.02122359]\n",
    "\n",
    "bins_lon = [-87.9136246 , -87.76550161, -87.68751552, -87.67161972,\n",
    "       -87.66341641, -87.65599818, -87.65253448, -87.64629348,\n",
    "       -87.642649  , -87.63784421, -87.63330804, -87.63274649,\n",
    "       -87.63186395, -87.62887416, -87.62621491, -87.62519214,\n",
    "       -87.62099291, -87.62076287, -87.61886836, -87.54093551]\n",
    "\n",
    "RAW_DATA_FEATURE_SPEC = dict([\n",
    "        ('fare', tf.io.VarLenFeature(tf.float32)),\n",
    "        ('trip_start_month', tf.io.VarLenFeature(tf.string)),\n",
    "        ('trip_start_hour', tf.io.VarLenFeature(tf.string)),\n",
    "        ('trip_start_day', tf.io.VarLenFeature(tf.string)),\n",
    "        ('pickup_latitude', tf.io.FixedLenFeature([], tf.float32)),\n",
    "        ('pickup_longitude', tf.io.FixedLenFeature([], tf.float32)),\n",
    "        ('dropoff_latitude', tf.io.FixedLenFeature([], tf.float32)),\n",
    "        ('dropoff_longitude', tf.io.FixedLenFeature([], tf.float32)),\n",
    "        ])\n",
    "    \n",
    "###############################\n",
    "##Feature engineering functions\n",
    "def feature_engg_features(features):\n",
    "  #Add new features( Non-TFT transformation ) -> Just for study purposes\n",
    "  features['distance'] = ((features['pickup_latitude'] - features['dropoff_latitude'])**2 +  (features['pickup_longitude'] - features['dropoff_longitude'])**2)**0.5\n",
    "\n",
    "  return(features)\n",
    "\n",
    "#To be called from TF\n",
    "def feature_engg(features, label):\n",
    "  #Add new features\n",
    "  features = feature_engg_features(features)\n",
    "\n",
    "  return(features, label)\n",
    "\n",
    "###############################\n",
    "###Data Input pipeline function\n",
    "\n",
    "def make_input_fn(filename, mode, vnum_epochs = None, batch_size = 512):\n",
    "    def _input_fn(v_test=False):     \n",
    "        # Create list of files that match pattern\n",
    "        file_list = tf.io.gfile.glob(filename)\n",
    "\n",
    "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "            num_epochs = vnum_epochs # indefinitely\n",
    "        else:\n",
    "            num_epochs = 1 # end-of-input after this        \n",
    "        \n",
    "        # Create dataset from file list\n",
    "        dataset = tf.compat.v1.data.experimental.make_csv_dataset(file_list,\n",
    "                                                   batch_size=batch_size,\n",
    "                                                   column_names=CSV_COLUMNS,\n",
    "                                                   column_defaults=DEFAULTS,\n",
    "                                                   label_name=LABEL_COLUMN,\n",
    "                                                   num_epochs = num_epochs,\n",
    "                                                   num_parallel_reads=30)\n",
    "        \n",
    "        dataset = dataset.prefetch(buffer_size = batch_size)\n",
    "\n",
    "        #Feature engineering\n",
    "        dataset = dataset.map(feature_engg)\n",
    "\n",
    "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "            num_epochs = vnum_epochs # indefinitely\n",
    "            dataset = dataset.shuffle(buffer_size = batch_size)\n",
    "        else:\n",
    "            num_epochs = 1 # end-of-input after this\n",
    "\n",
    "        dataset = dataset.repeat(num_epochs)       \n",
    "        \n",
    "        #Begins - Uncomment for testing only -----------------------------------------------------<\n",
    "        if v_test == True:\n",
    "          print(next(dataset.__iter__()))\n",
    "          \n",
    "        #End - Uncomment for testing only -----------------------------------------------------<\n",
    "        return dataset\n",
    "    return _input_fn\n",
    "\n",
    "# Define feature columns(Including feature engineered ones )\n",
    "# These are the features which come from the TF Data pipeline\n",
    "def create_feature_cols():\n",
    "    #Keras format features\n",
    "    # k_pickup_longitude_scaled = tf.keras.Input(name='pickup_longitude_scaled', shape=(1,), dtype=tf.float32, sparse=False) #-> Sparse because VarLenFeature\n",
    "    # k_pickup_latitude_scaled = tf.keras.Input(name='pickup_latitude_scaled', shape=(1,), dtype=tf.float32, sparse=False) #-> Sparse because VarLenFeature\n",
    "    k_month = tf.keras.Input(name='trip_start_month', shape=(1,), dtype=tf.string, sparse=False)\n",
    "    k_hour  = tf.keras.Input(name='trip_start_hour', shape=(1,), dtype=tf.string, sparse=False)\n",
    "    k_day  = tf.keras.Input(name='trip_start_day', shape=(1,), dtype=tf.string, sparse=False)\n",
    "    k_picklat  = tf.keras.Input(name='pickup_latitude', shape=(1,), dtype=tf.float32, sparse=False)\n",
    "    k_picklon  = tf.keras.Input(name='pickup_longitude', shape=(1,), dtype=tf.float32, sparse=False)\n",
    "    k_droplat  = tf.keras.Input(name='dropoff_latitude', shape=(1,), dtype=tf.float32, sparse=False)\n",
    "    k_droplon  = tf.keras.Input(name='dropoff_longitude', shape=(1,), dtype=tf.float32, sparse=False)\n",
    "    k_distance  = tf.keras.Input(name='distance', shape=(1,), dtype=tf.float32, sparse=False)\n",
    "    keras_dict_input = {'trip_start_month': k_month, 'trip_start_hour': k_hour, 'trip_start_day' : k_day,\n",
    "                        'pickup_latitude': k_picklat, 'pickup_longitude': k_picklon,\n",
    "                        'dropoff_latitude': k_droplat, 'dropoff_longitude': k_droplon, 'distance' : k_distance,\n",
    "                        # 'pickup_longitude_scaled': k_pickup_longitude_scaled,\n",
    "                        # 'pickup_latitude_scaled' : k_pickup_latitude_scaled\n",
    "                        }\n",
    "\n",
    "    return({'K' : keras_dict_input})\n",
    "\n",
    "def create_keras_model(params, feature_cols):\n",
    "    METRICS = [\n",
    "            keras.metrics.RootMeanSquaredError(name='rmse'),\n",
    "            keras.metrics.MeanAbsolutePercentageError(name='mape')\n",
    "    ]\n",
    "\n",
    "    #Input layers\n",
    "    input_feats = []\n",
    "    for inp in feature_cols['K'].keys():\n",
    "      input_feats.append(feature_cols['K'][inp])\n",
    "\n",
    "    ##Input processing\n",
    "    ##https://keras.io/examples/structured_data/structured_data_classification_from_scratch/\n",
    "    ##https://github.com/tensorflow/community/blob/master/rfcs/20191212-keras-categorical-inputs.md\n",
    "\n",
    "    ##Handle categorical attributes( One-hot encoding )\n",
    "    cat_day = tf.keras.layers.experimental.preprocessing.StringLookup(vocabulary = ['0','1','2','3','4','5','6','7'], mask_token=None, oov_token = '0')(feature_cols['K']['trip_start_day'])\n",
    "    cat_day = tf.keras.layers.experimental.preprocessing.CategoryEncoding(num_tokens=8)(cat_day)\n",
    "\n",
    "    cat_hour = tf.keras.layers.experimental.preprocessing.StringLookup(vocabulary=['1','2','3','4','5','6','7','8'\n",
    "                                                                                      '9','10','11','12','13','14','15','16',\n",
    "                                                                                      '17','18','19','20','21','22','23','0'\n",
    "                                                                                      ], mask_token=None)(feature_cols['K']['trip_start_hour'])\n",
    "    cat_hour = tf.keras.layers.experimental.preprocessing.CategoryEncoding(num_tokens=24)(cat_hour)\n",
    "\n",
    "    cat_month = tf.keras.layers.experimental.preprocessing.StringLookup(vocabulary=['1','2','3','4','5','6','7','8'\n",
    "                                                                                      '9','10','11','12'], mask_token=None)(feature_cols['K']['trip_start_month'])\n",
    "    cat_month = tf.keras.layers.experimental.preprocessing.CategoryEncoding(num_tokens=12)(cat_month)\n",
    "\n",
    "    # cat_company = tf.keras.layers.experimental.preprocessing.StringLookup(vocabulary=df['company'].unique(), mask_token=None)(feature_cols['K']['company'])\n",
    "    # cat_company = tf.keras.layers.experimental.preprocessing.CategoryEncoding(num_tokens=len(df['company'].unique()))(cat_company)\n",
    "\n",
    "    ##Binning\n",
    "    bins_pickup_lat = tf.keras.layers.experimental.preprocessing.Discretization(bins = bins_lat)(feature_cols['K']['pickup_latitude'])\n",
    "    cat_pickup_lat = tf.keras.layers.experimental.preprocessing.CategoryEncoding(len(bins_lat)+1)(bins_pickup_lat)\n",
    "\n",
    "    bins_pickup_lon = tf.keras.layers.experimental.preprocessing.Discretization(bins = bins_lon)(feature_cols['K']['pickup_longitude'])\n",
    "    cat_pickup_lon = tf.keras.layers.experimental.preprocessing.CategoryEncoding(len(bins_lon)+1)(bins_pickup_lon)\n",
    "\n",
    "    bins_drop_lat = tf.keras.layers.experimental.preprocessing.Discretization(bins = bins_lat)(feature_cols['K']['dropoff_latitude'])\n",
    "    cat_drop_lat = tf.keras.layers.experimental.preprocessing.CategoryEncoding(len(bins_lat)+1)(bins_drop_lat)\n",
    "\n",
    "    bins_drop_lon = tf.keras.layers.experimental.preprocessing.Discretization(bins = bins_lon)(feature_cols['K']['dropoff_longitude'])\n",
    "    cat_drop_lon = tf.keras.layers.experimental.preprocessing.CategoryEncoding(len(bins_lon)+1)(bins_drop_lon)\n",
    "\n",
    "    ##Categorical cross\n",
    "    cross_day_hour = tf.keras.layers.experimental.preprocessing.CategoryCrossing()([cat_day, cat_hour])\n",
    "    # hash_cross_day_hour = tf.keras.layers.experimental.preprocessing.HashedCrossing(num_bins=24 * 8, output_mode='one_hot')(cross_day_hour)\n",
    "\n",
    "#     cross_pick_lon_lat = tf.keras.layers.experimental.preprocessing.CategoryCrossing()([cat_pickup_lat, cat_pickup_lon])\n",
    "#     hash_cross_pick_lon_lat = tf.keras.layers.experimental.preprocessing.HashedCrossing(num_bins=(len(bins_lat) + 1) ** 2)(cross_pick_lon_lat)\n",
    "\n",
    "#     cross_drop_lon_lat = tf.keras.layers.experimental.preprocessing.CategoryCrossing()([cat_drop_lat, cat_drop_lon])\n",
    "#     hash_cross_drop_lon_lat = tf.keras.layers.experimental.preprocessing.HashedCrossing(num_bins=(len(bins_lat) + 1) ** 2)(cross_drop_lon_lat)\n",
    "\n",
    "    # Cross to embedding\n",
    "#     embed_cross_pick_lon_lat = tf.keras.layers.Embedding(((len(bins_lat) + 1) ** 2), 4)(hash_cross_pick_lon_lat)\n",
    "#     embed_cross_pick_lon_lat = tf.reduce_sum(embed_cross_pick_lon_lat, axis=-2)\n",
    "\n",
    "#     embed_cross_drop_lon_lat = tf.keras.layers.Embedding(((len(bins_lat) + 1) ** 2), 4)(hash_cross_drop_lon_lat)\n",
    "#     embed_cross_drop_lon_lat = tf.reduce_sum(embed_cross_drop_lon_lat, axis=-2)\n",
    "\n",
    "    # Also pass time attributes as Deep signal( Cast to integer )\n",
    "    int_trip_start_day = tf.strings.to_number(feature_cols['K']['trip_start_day'], tf.float32)\n",
    "    int_trip_start_hour = tf.strings.to_number(feature_cols['K']['trip_start_hour'], tf.float32)\n",
    "    int_trip_start_month = tf.strings.to_number(feature_cols['K']['trip_start_month'], tf.float32)\n",
    "\n",
    "    #Add feature engineered columns - LAMBDA layer\n",
    "\n",
    "    ###Create MODEL\n",
    "    ####Concatenate all features( Numerical input )\n",
    "    x_input_numeric = tf.keras.layers.concatenate([\n",
    "                    feature_cols['K']['pickup_latitude'], feature_cols['K']['pickup_longitude'],\n",
    "                    feature_cols['K']['dropoff_latitude'], feature_cols['K']['dropoff_longitude'],\n",
    "                    # feature_cols['K']['pickup_latitude_scaled'], feature_cols['K']['pickup_longitude_scaled'],\n",
    "                    feature_cols['K']['distance'], \n",
    "                    # embed_cross_pick_lon_lat, embed_cross_drop_lon_lat,\n",
    "                    int_trip_start_day, int_trip_start_hour, int_trip_start_month\n",
    "                    ])\n",
    "\n",
    "    #DEEP - This Dense layer connects to input layer - Numeric Data\n",
    "    # x_numeric = tf.keras.layers.Dense(32, activation='selu', kernel_initializer=\"lecun_normal\")(x_input_numeric)\n",
    "    x_numeric = tf.keras.layers.BatchNormalization()(x_input_numeric)\n",
    "\n",
    "    ####Concatenate all Categorical features( Categorical converted )\n",
    "    x_categ = tf.keras.layers.concatenate([\n",
    "                    cat_month, #cat_cross_day_hour, \n",
    "                    cat_pickup_lat, cat_pickup_lon,\n",
    "                    cat_drop_lat, cat_drop_lon\n",
    "                    ])\n",
    "    \n",
    "    #WIDE - This Dense layer connects to input layer - Categorical Data\n",
    "    # x_categ = tf.keras.layers.Dense(32, activation='selu', kernel_initializer=\"lecun_normal\")(x_input_categ)\n",
    "\n",
    "    ####Concatenate both Wide and Deep layers\n",
    "    x = tf.keras.layers.concatenate([x_categ, x_numeric])\n",
    "\n",
    "    for l_ in range(params['hidden_layers']):\n",
    "        x = tf.keras.layers.Dense(32, activation='selu', kernel_initializer=\"lecun_normal\",\n",
    "                                  activity_regularizer=tf.keras.regularizers.l2(0.00001))(x)\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "\n",
    "    #Final Layer\n",
    "    out = tf.keras.layers.Dense(1, activation='linear')(x)\n",
    "    model = tf.keras.Model(input_feats, out)\n",
    "\n",
    "    #Set optimizer\n",
    "    opt = tf.keras.optimizers.Adam(lr= params['lr'])\n",
    "\n",
    "    #Compile model\n",
    "    model.compile(loss='mean_squared_error',  optimizer=opt, metrics = METRICS)\n",
    "\n",
    "    #Print Summary\n",
    "    print(model.summary())\n",
    "    return model\n",
    "\n",
    "def keras_train_and_evaluate(model, train_dataset, validation_dataset, epochs=100):\n",
    "    #Add callbacks\n",
    "    reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
    "                                patience=5, min_lr=0.00001, verbose = 1)\n",
    "\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"logs/\")\n",
    "\n",
    "    #Train and Evaluate\n",
    "    out = model.fit(train_dataset, \n",
    "                  validation_data = validation_dataset,\n",
    "                  epochs=epochs,\n",
    "                  # validation_steps = 3,   ###Keep this none for running evaluation on full EVAL data every epoch\n",
    "                  steps_per_epoch = 100,   ###Has to be passed - Cant help it :) [ Number of batches per epoch ]\n",
    "                  callbacks=[reduce_lr, #modelsave_callback, \n",
    "                             tensorboard_callback, \n",
    "                             keras.callbacks.EarlyStopping(patience=20, restore_best_weights=True, verbose=True)]\n",
    "                  )\n",
    "\n",
    "    #Best index\n",
    "    best_epoch = np.argmin(out.history['val_rmse'])\n",
    "\n",
    "    return (out.history['val_rmse'][best_epoch], out.history['val_mape'][best_epoch])\n",
    "  \n",
    "def main(args):\n",
    "    @tf.function\n",
    "    def serving(dropoff_latitude, dropoff_longitude, pickup_latitude, pickup_longitude, trip_start_day, trip_start_hour, trip_start_month):\n",
    "        #Params coming in request\n",
    "        features = {\n",
    "            'dropoff_latitude': dropoff_latitude,\n",
    "            'dropoff_longitude': dropoff_longitude,\n",
    "            'pickup_latitude': pickup_latitude,\n",
    "            'pickup_longitude': pickup_longitude,\n",
    "            'trip_start_day': trip_start_day,\n",
    "            'trip_start_hour': trip_start_hour,\n",
    "            'trip_start_month': trip_start_month\n",
    "        }\n",
    "\n",
    "        #Add TFT transformations\n",
    "        raw_features = {}\n",
    "        for key, val in features.items():\n",
    "          if key not in RAW_DATA_FEATURE_SPEC:\n",
    "            continue\n",
    "          if isinstance(RAW_DATA_FEATURE_SPEC[key], tf.io.VarLenFeature):\n",
    "            raw_features[key] = tf.RaggedTensor.from_tensor(\n",
    "                tf.expand_dims(val, -1)).to_sparse()\n",
    "            continue\n",
    "          raw_features[key] = val\n",
    "        # tft_new_features = tft_layer(raw_features)\n",
    "\n",
    "        # pickup_longitude_scaled = tft_new_features['pickup_longitude_scaled'] \n",
    "        # pickup_latitude_scaled = tft_new_features['pickup_latitude_scaled']\n",
    "        distance = ((features['pickup_latitude'] - features['dropoff_latitude'])**2 +  (features['pickup_longitude'] - features['dropoff_longitude'])**2)**0.5 ##tft_new_features['distance']\n",
    "\n",
    "        ##Feature engineering( calculate distance )\n",
    "        # distance = tf.cast( tf.sqrt((tf.abs(dropoff_latitude - pickup_latitude))**2 + (tf.abs(dropoff_longitude - pickup_longitude))**2), tf.float32)\n",
    "\n",
    "        #Params in request + New Feature engineering params\n",
    "        payload = {\n",
    "            'dropoff_latitude': dropoff_latitude,\n",
    "            'dropoff_longitude': dropoff_longitude,\n",
    "            'pickup_latitude': pickup_latitude,\n",
    "            'pickup_longitude': pickup_longitude,\n",
    "            'trip_start_day': trip_start_day,\n",
    "            'trip_start_hour': trip_start_hour,\n",
    "            'trip_start_month': trip_start_month,\n",
    "            'distance': distance,\n",
    "            # 'pickup_longitude_scaled': pickup_longitude_scaled,\n",
    "            # 'pickup_latitude_scaled': pickup_latitude_scaled,\n",
    "        }\n",
    "\n",
    "        ## Predict\n",
    "        ##IF THERE IS AN ERROR IN NUMBER OF PARAMS PASSED HERE OR DATA TYPE THEN IT GIVES ERROR, \"COULDN'T COMPUTE OUTPUT TENSOR\"\n",
    "        predictions = m_(payload)\n",
    "        return predictions\n",
    "\n",
    "    #####MAIN STARTS\n",
    "    ##Device Strategy\n",
    "    device = \"cpu\"\n",
    "    if device == \"tpu\":\n",
    "      resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])\n",
    "      tf.config.experimental_connect_to_cluster(resolver)\n",
    "      # This is the TPU initialization code that has to be at the beginning.\n",
    "      tf.tpu.experimental.initialize_tpu_system(resolver)\n",
    "      strategy = tf.distribute.experimental.TPUStrategy(resolver)\n",
    "    else:\n",
    "      strategy = tf.distribute.MultiWorkerMirroredStrategy()\n",
    "    \n",
    "    #Create FC\n",
    "    create_feature_cols()\n",
    "\n",
    "    #Create model\n",
    "    params_default = {\n",
    "        'lr' : args.lr,\n",
    "        'beta_1' : 0.99,\n",
    "        'beta_2' : 0.999,\n",
    "        'epsilon' : 1e-08,\n",
    "        'decay' : 0.01,\n",
    "        'hidden_layers' : args.hidden_layers\n",
    "    }\n",
    "\n",
    "    #Create dataset input functions\n",
    "    train_dataset = make_input_fn(filename = args.train_file,\n",
    "                        mode = tf.estimator.ModeKeys.TRAIN,\n",
    "                        batch_size = 128)()\n",
    "\n",
    "    validation_dataset = make_input_fn(filename = args.eval_file,\n",
    "                        mode = tf.estimator.ModeKeys.EVAL,\n",
    "                        batch_size = 512)()\n",
    "\n",
    "    m_ = create_keras_model(params = params_default, feature_cols = create_feature_cols())\n",
    "    # tf.keras.utils.plot_model(m_, show_shapes=True, rankdir=\"LR\")\n",
    "\n",
    "    #Train Model\n",
    "    rmse, mape = keras_train_and_evaluate(m_, train_dataset, validation_dataset, args.epochs)\n",
    "    print(\"Final Val RMSE: \", rmse)\n",
    "    print(\"Final Val MAPE: \", mape)\n",
    "    \n",
    "    #Log metrics to Vertex MLMD\n",
    "    aiplatform.init(\n",
    "        project=args.project,\n",
    "        location=args.mlmd_region,\n",
    "        experiment=args.experiment_name\n",
    "    )\n",
    "    \n",
    "    aiplatform.start_run(\"1\")  # Change this to your desired run name\n",
    "    metrics = {\"rmse\": rmse, \"mape\": mape}\n",
    "    aiplatform.log_metrics(metrics)\n",
    "    \n",
    "    #Report metrics for HPT\n",
    "    hpt = hypertune.HyperTune()\n",
    "    hpt.report_hyperparameter_tuning_metric(\n",
    "      hyperparameter_metric_tag='eval_rmse',\n",
    "      metric_value=rmse,\n",
    "      global_step=args.epochs)\n",
    "    \n",
    "    #Save model\n",
    "    serving = serving.get_concrete_function(trip_start_day=tf.TensorSpec([None,], dtype= tf.string, name='trip_start_day'), \n",
    "                                            trip_start_hour=tf.TensorSpec([None,], dtype= tf.string, name='trip_start_hour'),\n",
    "                                            trip_start_month=tf.TensorSpec([None], dtype= tf.string, name='trip_start_month'), \n",
    "                                            dropoff_latitude=tf.TensorSpec([None,], dtype= tf.float32, name='dropoff_latitude'),\n",
    "                                            dropoff_longitude=tf.TensorSpec([None,], dtype= tf.float32, name='dropoff_longitude'), \n",
    "                                            pickup_latitude=tf.TensorSpec([None,], dtype= tf.float32, name='pickup_latitude'),\n",
    "                                            pickup_longitude=tf.TensorSpec([None,], dtype= tf.float32, name='pickup_longitude')\n",
    "                                            )\n",
    "\n",
    "    ##model Saving\n",
    "    try:\n",
    "        model_save_location = os.environ['AIP_MODEL_DIR']\n",
    "        print(\"Saving at AIP_MODEL_DIR \", model_save_location) \n",
    "    except:\n",
    "        version = \"1\"  #{'serving_default': call_output}\n",
    "        model_save_location = args.model_save_location + version\n",
    "        print(\"Saving at custom dir \", model_save_location)        \n",
    "        \n",
    "    print(\"Saving model...\")\n",
    "    \n",
    "    tf.saved_model.save(\n",
    "        m_,\n",
    "        model_save_location,\n",
    "        signatures=serving\n",
    "    )\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    ##Parse Arguments\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "      '--train_file', required=True, type=str, help='Training file')\n",
    "    parser.add_argument(\n",
    "      '--eval_file', required=True, type=str, help='Eval file')\n",
    "    parser.add_argument(\n",
    "      '--model_save_location', required=True, type=str, help='Model save location')\n",
    "    parser.add_argument(\n",
    "      '--epochs', required=False, type=int, help='Epochs', default=100)\n",
    "    parser.add_argument(\n",
    "      '--lr', required=False, type=float, help='Learning Rate', default=0.001)\n",
    "    parser.add_argument(\n",
    "      '--hidden_layers', required=False, type=int, help='Hidden layers', default=1)\n",
    "    parser.add_argument(\n",
    "      '--experiment_name', required=True, type=str, help='Experiment ID')\n",
    "    parser.add_argument(\n",
    "      '--mlmd_region', required=True, type=str, help='Region')\n",
    "    parser.add_argument(\n",
    "      '--project', required=True, type=str, help='Project')\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    #Run Main Trainer\n",
    "    main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "804789fd-87f6-4a31-9de3-1842b12dc041",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-24 18:50:55.307117: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n",
      "INFO:tensorflow:Single-worker MultiWorkerMirroredStrategy with local_devices = ('/device:CPU:0',), communication = CommunicationImplementation.AUTO\n",
      "WARNING:tensorflow:bins is deprecated, please use bin_boundaries or num_bins instead.\n",
      "WARNING:tensorflow:bins is deprecated, please use bin_boundaries or num_bins instead.\n",
      "WARNING:tensorflow:bins is deprecated, please use bin_boundaries or num_bins instead.\n",
      "WARNING:tensorflow:bins is deprecated, please use bin_boundaries or num_bins instead.\n",
      "/opt/conda/lib/python3.7/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " trip_start_month (InputLayer)  [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " pickup_latitude (InputLayer)   [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " pickup_longitude (InputLayer)  [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " dropoff_latitude (InputLayer)  [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " dropoff_longitude (InputLayer)  [(None, 1)]         0           []                               \n",
      "                                                                                                  \n",
      " trip_start_day (InputLayer)    [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " trip_start_hour (InputLayer)   [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " string_lookup_2 (StringLookup)  (None, 1)           0           ['trip_start_month[0][0]']       \n",
      "                                                                                                  \n",
      " discretization (Discretization  (None, 1)           0           ['pickup_latitude[0][0]']        \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " discretization_1 (Discretizati  (None, 1)           0           ['pickup_longitude[0][0]']       \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " discretization_2 (Discretizati  (None, 1)           0           ['dropoff_latitude[0][0]']       \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " discretization_3 (Discretizati  (None, 1)           0           ['dropoff_longitude[0][0]']      \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " distance (InputLayer)          [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " tf.strings.to_number (TFOpLamb  (None, 1)           0           ['trip_start_day[0][0]']         \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " tf.strings.to_number_1 (TFOpLa  (None, 1)           0           ['trip_start_hour[0][0]']        \n",
      " mbda)                                                                                            \n",
      "                                                                                                  \n",
      " tf.strings.to_number_2 (TFOpLa  (None, 1)           0           ['trip_start_month[0][0]']       \n",
      " mbda)                                                                                            \n",
      "                                                                                                  \n",
      " category_encoding_2 (CategoryE  (None, 12)          0           ['string_lookup_2[0][0]']        \n",
      " ncoding)                                                                                         \n",
      "                                                                                                  \n",
      " category_encoding_3 (CategoryE  (None, 21)          0           ['discretization[0][0]']         \n",
      " ncoding)                                                                                         \n",
      "                                                                                                  \n",
      " category_encoding_4 (CategoryE  (None, 21)          0           ['discretization_1[0][0]']       \n",
      " ncoding)                                                                                         \n",
      "                                                                                                  \n",
      " category_encoding_5 (CategoryE  (None, 21)          0           ['discretization_2[0][0]']       \n",
      " ncoding)                                                                                         \n",
      "                                                                                                  \n",
      " category_encoding_6 (CategoryE  (None, 21)          0           ['discretization_3[0][0]']       \n",
      " ncoding)                                                                                         \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 8)            0           ['pickup_latitude[0][0]',        \n",
      "                                                                  'pickup_longitude[0][0]',       \n",
      "                                                                  'dropoff_latitude[0][0]',       \n",
      "                                                                  'dropoff_longitude[0][0]',      \n",
      "                                                                  'distance[0][0]',               \n",
      "                                                                  'tf.strings.to_number[0][0]',   \n",
      "                                                                  'tf.strings.to_number_1[0][0]', \n",
      "                                                                  'tf.strings.to_number_2[0][0]'] \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 96)           0           ['category_encoding_2[0][0]',    \n",
      "                                                                  'category_encoding_3[0][0]',    \n",
      "                                                                  'category_encoding_4[0][0]',    \n",
      "                                                                  'category_encoding_5[0][0]',    \n",
      "                                                                  'category_encoding_6[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 8)           32          ['concatenate[0][0]']            \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate)    (None, 104)          0           ['concatenate_1[0][0]',          \n",
      "                                                                  'batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 32)           3360        ['concatenate_2[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 32)          128         ['dense[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 32)           1056        ['batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 32)          128         ['dense_1[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 1)            33          ['batch_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 4,737\n",
      "Trainable params: 4,593\n",
      "Non-trainable params: 144\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/3\n",
      "100/100 [==============================] - 4s 22ms/step - loss: 163.2822 - rmse: 12.7782 - mape: 1960723.8750 - val_loss: 209.1228 - val_rmse: 14.4606 - val_mape: 316929.0312 - lr: 0.0010\n",
      "Epoch 2/3\n",
      "100/100 [==============================] - 2s 15ms/step - loss: 106.7627 - rmse: 10.3326 - mape: 3067184.7500 - val_loss: 665.8125 - val_rmse: 25.8031 - val_mape: 17742920.0000 - lr: 0.0010\n",
      "Epoch 3/3\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 77.5418 - rmse: 8.8058 - mape: 4493270.0000 - val_loss: 330.1829 - val_rmse: 18.1708 - val_mape: 6733896.0000 - lr: 0.0010\n",
      "Final Val RMSE:  14.46063232421875\n",
      "Final Val MAPE:  316929.03125\n",
      "Saving at custom dir  gs://gcs-hasanrafiq-test-331814/ml_data/taxi_dataset/model/1\n",
      "Saving model...\n",
      "2022-01-24 18:51:06.101242: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "INFO:tensorflow:Assets written to: gs://gcs-hasanrafiq-test-331814/ml_data/taxi_dataset/model/1/assets\n"
     ]
    }
   ],
   "source": [
    "##Test the job\n",
    "!python task.py \\\n",
    "--train_file='gs://gcs-hasanrafiq-test-331814/ml_data/taxi_dataset/train.csv' \\\n",
    "--eval_file='gs://gcs-hasanrafiq-test-331814/ml_data/taxi_dataset/eval.csv' \\\n",
    "--model_save_location='gs://gcs-hasanrafiq-test-331814/ml_data/taxi_dataset/model/' \\\n",
    "--experiment_name='custom-model-test' \\\n",
    "--mlmd_region='us-central1' \\\n",
    "--project='hasanrafiq-test-331814' \\\n",
    "--epochs=3 \\\n",
    "--lr=0.001 \\\n",
    "--hidden_layers=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "012e3d4b-62fb-4a7f-8cc9-b86546dad172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting setup.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile setup.py\n",
    "\n",
    "from setuptools import find_packages\n",
    "from setuptools import setup\n",
    "\n",
    "REQUIRED_PACKAGES = ['absl-py','pandas','google-cloud-aiplatform',\n",
    "                     'google-cloud','google-cloud-storage','google-cloud-firestore','google-api-python-client', 'google-auth']\n",
    "\n",
    "setup(\n",
    "    name='trainer',\n",
    "    version='0.1',\n",
    "    install_requires=REQUIRED_PACKAGES,\n",
    "    packages=find_packages(),\n",
    "    include_package_data=True,\n",
    "    description='Hasan - Vertex AI Taxi Trainer Job'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef446d98-9cef-4610-aed1-58158a8a0227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting __init__.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile __init__.py\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7ac2864-e381-4782-bc2e-8abcde58ba70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running sdist\n",
      "running egg_info\n",
      "creating trainer.egg-info\n",
      "writing trainer.egg-info/PKG-INFO\n",
      "writing dependency_links to trainer.egg-info/dependency_links.txt\n",
      "writing requirements to trainer.egg-info/requires.txt\n",
      "writing top-level names to trainer.egg-info/top_level.txt\n",
      "writing manifest file 'trainer.egg-info/SOURCES.txt'\n",
      "reading manifest file 'trainer.egg-info/SOURCES.txt'\n",
      "writing manifest file 'trainer.egg-info/SOURCES.txt'\n",
      "running check\n",
      "warning: check: missing required meta-data: url\n",
      "\n",
      "warning: check: missing meta-data: either (author and author_email) or (maintainer and maintainer_email) must be supplied\n",
      "\n",
      "creating trainer-0.1\n",
      "creating trainer-0.1/components\n",
      "creating trainer-0.1/pipelines\n",
      "creating trainer-0.1/trainer\n",
      "creating trainer-0.1/trainer.egg-info\n",
      "copying files to trainer-0.1...\n",
      "copying README.md -> trainer-0.1\n",
      "copying setup.py -> trainer-0.1\n",
      "copying components/__init__.py -> trainer-0.1/components\n",
      "copying components/hpt.py -> trainer-0.1/components\n",
      "copying components/regression_eval_model.py -> trainer-0.1/components\n",
      "copying pipelines/__init__.py -> trainer-0.1/pipelines\n",
      "copying pipelines/train_pipeline.py -> trainer-0.1/pipelines\n",
      "copying trainer/__init__.py -> trainer-0.1/trainer\n",
      "copying trainer/task.py -> trainer-0.1/trainer\n",
      "copying trainer.egg-info/PKG-INFO -> trainer-0.1/trainer.egg-info\n",
      "copying trainer.egg-info/SOURCES.txt -> trainer-0.1/trainer.egg-info\n",
      "copying trainer.egg-info/dependency_links.txt -> trainer-0.1/trainer.egg-info\n",
      "copying trainer.egg-info/requires.txt -> trainer-0.1/trainer.egg-info\n",
      "copying trainer.egg-info/top_level.txt -> trainer-0.1/trainer.egg-info\n",
      "Writing trainer-0.1/setup.cfg\n",
      "creating dist\n",
      "Creating tar archive\n",
      "removing 'trainer-0.1' (and everything under it)\n",
      "Copying file://dist/trainer-0.1.tar.gz [Content-Type=application/x-tar]...\n",
      "/ [1 files][ 10.3 KiB/ 10.3 KiB]                                                \n",
      "Operation completed over 1 objects/10.3 KiB.                                     \n"
     ]
    }
   ],
   "source": [
    " # Create ML train package\n",
    "!rm -rf dist*\n",
    "!rm -rf trainer*\n",
    "!rm -rf trainer/\n",
    "!rm -rf dist/\n",
    "\n",
    "!mkdir trainer/\n",
    "!cp task.py trainer/\n",
    "!cp __init__.py trainer/\n",
    "!python setup.py sdist\n",
    "\n",
    "# Copy trainer.gz to GCS training path\n",
    "!gsutil cp dist/trainer-0.1.tar.gz gs://gcs-hasanrafiq-test-331814/ml_data/taxi_dataset/ml_scripts/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7de4c4-0b25-49c8-946c-30ec28faba4c",
   "metadata": {},
   "source": [
    "### Step 2 -> Now build pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "17ab2d0c-c027-43c6-9ef6-15cc73242b45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting pipelines/train_pipeline.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile pipelines/train_pipeline.py\n",
    "import os\n",
    "\n",
    "import kfp\n",
    "import time\n",
    "from kfp.v2 import compiler, dsl\n",
    "from kfp.v2.dsl import component, pipeline, Artifact, ClassificationMetrics, Input, Output, Model, Metrics\n",
    "\n",
    "from google.cloud import aiplatform\n",
    "from google_cloud_pipeline_components import aiplatform as gcc_aip\n",
    "from typing import NamedTuple, Dict\n",
    "\n",
    "from google.cloud import aiplatform\n",
    "\n",
    "#Main pipeline class\n",
    "class pipeline_controller():\n",
    "    def __init__(self, template_path, display_name, pipeline_root, project_id, region):\n",
    "        self.template_path = template_path\n",
    "        self.display_name = display_name\n",
    "        self.pipeline_root = pipeline_root\n",
    "        self.project_id = project_id\n",
    "        self.region = region\n",
    "    \n",
    "    def _build_compile_pipeline(self):\n",
    "        \"\"\"Method to build and compile pipeline\"\"\"\n",
    "        self.pipeline = self._get_pipeline()\n",
    "        compiler.Compiler().compile(\n",
    "            pipeline_func=self.pipeline, package_path=self.template_path\n",
    "        )\n",
    "        \n",
    "    def _submit_job(self):\n",
    "        \"\"\"Method to Submit ML Pipeline job\"\"\"\n",
    "        #Next, define the job:\n",
    "        ml_pipeline_job = aiplatform.PipelineJob(\n",
    "            display_name=self.display_name,\n",
    "            template_path=self.template_path,\n",
    "            pipeline_root=self.pipeline_root,\n",
    "            parameter_values={\"project\": self.project_id, \"display_name\": self.display_name},\n",
    "            enable_caching=False\n",
    "        )\n",
    "\n",
    "        #And finally, run the job:\n",
    "        ml_pipeline_job.submit()\n",
    "    \n",
    "    def _get_pipeline(self):\n",
    "        \"\"\"Main method to Create pipeline\"\"\"\n",
    "        @pipeline(name=self.display_name,\n",
    "                          pipeline_root=self.pipeline_root)\n",
    "        def pipeline_fn(\n",
    "            display_name: str = self.display_name,\n",
    "            project: str = self.project_id,\n",
    "            gcp_region: str = self.region,\n",
    "            api_endpoint: str = \"us-central1-aiplatform.googleapis.com\",\n",
    "            thresholds_dict_str: str = '{\"rmse_threshold\": 4.8}',\n",
    "        ):\n",
    "            \n",
    "            #Load all reusable custom components\n",
    "            eval_op = kfp.components.load_component('component_specs/regression_eval_model.yaml')\n",
    "            hpt_op = kfp.components.load_component('component_specs/hpt.yaml')\n",
    "\n",
    "            #STEP: For non Auto-ML call\n",
    "            hpt_args = ['--train_file', 'gs://gcs-hasanrafiq-test-331814/ml_data/taxi_dataset/train.csv',\n",
    "                         '--eval_file', 'gs://gcs-hasanrafiq-test-331814/ml_data/taxi_dataset/eval.csv',\n",
    "                         '--model_save_location', 'gs://gcs-hasanrafiq-test-331814/ml_data/taxi_dataset/model/',\n",
    "                         '--hidden_layers','2',\n",
    "                         '--experiment_name', \"{{$.inputs.parameters['display_name']}}-hpt-job\" ,  #This is also a way to pass a parameter as input\n",
    "                         '--mlmd_region', \"{{$.inputs.parameters['pipelineparam--gcp_region']}}\" ,\n",
    "                         '--project', \"{{$.inputs.parameters['project']}}\"\n",
    "                        ]\n",
    "            \n",
    "            #HPT\n",
    "            hpt_task = hpt_op(\n",
    "                project=project,\n",
    "                display_name=display_name,\n",
    "                service_account='318948681665-compute@developer.gserviceaccount.com',  ##Needed for Vertex MLMD access\n",
    "                executor_image_uri='us-docker.pkg.dev/vertex-ai/training/tf-cpu.2-7:latest',\n",
    "                package_uri=\"gs://gcs-hasanrafiq-test-331814/ml_data/taxi_dataset/ml_scripts/trainer-0.1.tar.gz\",\n",
    "                python_module=\"trainer.task\",\n",
    "                hpt_args=hpt_args,\n",
    "                metric_id=\"eval_rmse\",\n",
    "                goal=aiplatform.gapic.StudySpec.MetricSpec.GoalType.MINIMIZE,\n",
    "                location=gcp_region,\n",
    "                api_endpoint=str(f\"{gcp_region}-aiplatform.googleapis.com\")\n",
    "            )\n",
    "\n",
    "            #Training\n",
    "            training_args = ['--train_file', 'gs://gcs-hasanrafiq-test-331814/ml_data/taxi_dataset/train.csv',\n",
    "                             '--eval_file', 'gs://gcs-hasanrafiq-test-331814/ml_data/taxi_dataset/eval.csv',\n",
    "                             '--model_save_location', 'gs://gcs-hasanrafiq-test-331814/ml_data/taxi_dataset/model/',\n",
    "                             '--epochs', '10',\n",
    "                             '--hidden_layers', str(hpt_task.outputs[\"hidden_layers\"]),\n",
    "                             '--lr', str(hpt_task.outputs[\"lr\"]),\n",
    "                             '--experiment_name', str(f\"{display_name}-train-job\"),\n",
    "                             '--mlmd_region', str(gcp_region) ,\n",
    "                             '--project', str(project) ,\n",
    "                            ]\n",
    "            \n",
    "            training_op = gcc_aip.CustomPythonPackageTrainingJobRunOp(\n",
    "                            project=project,\n",
    "                            display_name=display_name,\n",
    "                            service_account='318948681665-compute@developer.gserviceaccount.com',  ##Needed for Vertex MLMD access\n",
    "                            python_package_gcs_uri=\"gs://gcs-hasanrafiq-test-331814/ml_data/taxi_dataset/ml_scripts/trainer-0.1.tar.gz\",\n",
    "                            staging_bucket='gs://cloud-ai-platform-35f2698c-5046-4c70-857e-14cb44e3950a/ml_staging',\n",
    "                            python_module_name=\"trainer.task\",\n",
    "                            container_uri='us-docker.pkg.dev/vertex-ai/training/tf-cpu.2-7:latest',\n",
    "                            model_serving_container_image_uri='us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-7:latest',\n",
    "                            model_display_name='vertex-customml-taxi',\n",
    "                            replica_count=1,\n",
    "                            location=gcp_region,\n",
    "                            machine_type=\"n1-standard-4\",\n",
    "                            args=training_args\n",
    "                          )\n",
    "\n",
    "            #Model Evaluation\n",
    "            model_eval_task = eval_op(\n",
    "                                    project,\n",
    "                                    gcp_region,\n",
    "                                    str(f\"{display_name}-train-job\"),\n",
    "                                    thresholds_dict_str,\n",
    "                                    training_op.outputs[\"model\"])\n",
    "                \n",
    "            with dsl.Condition(\n",
    "                model_eval_task.outputs[\"dep_decision\"] == \"true\",\n",
    "                name=\"deploy_decision\",\n",
    "            ):\n",
    "\n",
    "                endpoint_op = gcc_aip.EndpointCreateOp(\n",
    "                    project=project,\n",
    "                    location=gcp_region,\n",
    "                    display_name=\"vertex-customml-taxi\",\n",
    "                )\n",
    "\n",
    "                gcc_aip.ModelDeployOp(\n",
    "                    model=training_op.outputs[\"model\"],\n",
    "                    endpoint=endpoint_op.outputs[\"endpoint\"],\n",
    "                    dedicated_resources_min_replica_count=1,\n",
    "                    dedicated_resources_max_replica_count=1,\n",
    "                    dedicated_resources_machine_type=\"n1-standard-4\",\n",
    "                )\n",
    "            \n",
    "        return pipeline_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dffb437c-a33b-4490-9fc4-4f3f095763e1",
   "metadata": {},
   "source": [
    "#### Build and Upload image to GAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7bcb7a17-8179-4515-b479-c23990c73f9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending build context to Docker daemon  2.129MB\n",
      "Step 1/4 : FROM python:3.7-slim AS builder\n",
      " ---> d3c9ad326043\n",
      "Step 2/4 : COPY requirements.txt .\n",
      " ---> Using cache\n",
      " ---> 71ab63d04aea\n",
      "Step 3/4 : RUN pip install -r requirements.txt\n",
      " ---> Using cache\n",
      " ---> f441fe232539\n",
      "Step 4/4 : COPY . .\n",
      " ---> 02049d964be3\n",
      "Successfully built 02049d964be3\n",
      "Successfully tagged us-central1-docker.pkg.dev/hasanrafiq-test-331814/vertex-customml-pipeline-test/latest:latest\n",
      "The push refers to repository [us-central1-docker.pkg.dev/hasanrafiq-test-331814/vertex-customml-pipeline-test/latest]\n",
      "b6fd794eea2e: Preparing\n",
      "71147fe53e89: Preparing\n",
      "c5c70a818500: Preparing\n",
      "25ad0307b4c1: Preparing\n",
      "874b45955cb1: Preparing\n",
      "85c923303735: Preparing\n",
      "d0fa20bfdce7: Preparing\n",
      "2edcec3590a4: Preparing\n",
      "85c923303735: Waiting\n",
      "d0fa20bfdce7: Waiting\n",
      "2edcec3590a4: Waiting\n",
      "25ad0307b4c1: Layer already exists\n",
      "c5c70a818500: Layer already exists\n",
      "85c923303735: Layer already exists\n",
      "874b45955cb1: Layer already exists\n",
      "71147fe53e89: Layer already exists\n",
      "2edcec3590a4: Layer already exists\n",
      "d0fa20bfdce7: Layer already exists\n",
      "b6fd794eea2e: Pushed\n",
      "latest: digest: sha256:6075421ae3778e99dcbc2c101edc7fea569aa67d1040a6a642233ba79b4be604 size: 2000\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "docker build -t us-central1-docker.pkg.dev/hasanrafiq-test-331814/vertex-customml-pipeline-test/latest:latest -f Dockerfile .\n",
    "docker push us-central1-docker.pkg.dev/hasanrafiq-test-331814/vertex-customml-pipeline-test/latest:latest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c8e2ba-3917-4ea5-be1c-ea36b254dcfd",
   "metadata": {},
   "source": [
    "#### Directly test running of pipeline from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "14582539-343f-4be7-b4e1-82de3606b33a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:Creating PipelineJob\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.7/site-packages/kfp/v2/compiler/compiler.py:1266: FutureWarning: APIs imported from the v1 namespace (e.g. kfp.dsl, kfp.components, etc) will not be supported by the v2 compiler since v2.0.0\n",
      "  category=FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob created. Resource name: projects/318948681665/locations/us-central1/pipelineJobs/customml-taxi-training-20220124185124\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:To use this PipelineJob in another session:\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:pipeline_job = aiplatform.PipelineJob.get('projects/318948681665/locations/us-central1/pipelineJobs/customml-taxi-training-20220124185124')\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/customml-taxi-training-20220124185124?project=318948681665\n"
     ]
    }
   ],
   "source": [
    "#Initialize pipeline object\n",
    "from pipelines.train_pipeline import pipeline_controller\n",
    "import time\n",
    "\n",
    "PROJECT_ID = \"hasanrafiq-test-331814\"\n",
    "REGION=\"us-central1\"\n",
    "\n",
    "BUCKET_NAME=f\"gs://gcs-{PROJECT_ID}\"\n",
    "BUCKET_NAME\n",
    "\n",
    "PIPELINE_ROOT = f\"{BUCKET_NAME}/pipeline_root/\"\n",
    "PIPELINE_ROOT\n",
    "\n",
    "DISPLAY_NAME = 'hasantest-vertex-customml-pipeline{}'.format(str(int(time.time())))\n",
    "DISPLAY_NAME\n",
    "\n",
    "pipe = pipeline_controller(template_path=\"pipeline.json\",\n",
    "                           display_name=\"customml-taxi-training\", \n",
    "                           pipeline_root=PIPELINE_ROOT,\n",
    "                           project_id=PROJECT_ID,\n",
    "                           region=REGION)\n",
    "\n",
    "#Build and Compile pipeline\n",
    "pipe._build_compile_pipeline()\n",
    "\n",
    "# #Submit Job\n",
    "pipe._submit_job()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "29c0025b-9fe9-41ca-b2df-b4f866a1c911",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from typing import NamedTuple\n",
    "# import inspect\n",
    "\n",
    "# def test() -> NamedTuple(\"Outputs\", [(\"lr\", float), (\"hidden_layers\", float)]):\n",
    "#     return (1, 2)\n",
    "\n",
    "# inspect.signature(test).return_annotation._fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ef274ec7-5297-4e77-bdd6-a88372ad8aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(test())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc61102-28d0-4283-990d-70066c6d8a26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-7.m87",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-7:m87"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
